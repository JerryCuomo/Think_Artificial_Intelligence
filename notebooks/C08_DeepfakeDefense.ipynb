{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 8** - Multimedia and Deepfake Defense\n",
        "\n",
        "This notebook demonstrates the integration of AI in multimedia, featuring object detection and video annotation using open-source libraries like `Librosa`, `OpenCV`, `SpeechT5` and `YOLOv5`. With these tools, you’ll learn how to analyze video frames, detect objects, and overlay annotations, creating enhanced, interactive visual content. The modular design makes it easy to follow, adapt, and extend for custom applications. Libraries such as pandas and matplotlib add support for structured data handling and visualization.\n",
        "**Note**: GPU acceleration is recommended for optimal performance for some of the code cells that train AI models."
      ],
      "metadata": {
        "id": "9I6-BLIrnw8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 8-1: Download Audio Samples from Github\n",
        "This code prepares the environment by downloading required audio samples from GitHub. It includes Jerry’s podcast samples and non-Jerry audio files, ensuring they are available locally for training and testing purposes.\n",
        "\n",
        "**Note 1:** The download process can take a few mins.\n",
        "\n",
        "**Note 2:** Using WAV files with a sampling rate of 16kHz and Signed 16-bit PCM encoding ensures compatibility with SpeechT5. Consistent format avoids processing errors, maintains audio quality, and allows the model to generate accurate spectrograms. Variations in format can disrupt training and degrade synthesized speech quality."
      ],
      "metadata": {
        "id": "69VraYOeocNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "# Base GitHub repository URL for audio files\n",
        "BASE_URL = \"https://opensourceai-book.github.io/code/media/\"\n",
        "\n",
        "# List of Jerry's podcast audio samples for training and testing (Label 1)\n",
        "Jerry_Audio_Files = [\n",
        "    \"L1-Sample01-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample02-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample03-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample04-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample05-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample06-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample07-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample08-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample09-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample10-Jerry.wav\",  # Training sample\n",
        "    \"L1-Sample11-Jerry.wav\",  # Reserved for test\n",
        "    \"L1-Sample12-Jerry.wav\",  # Reserved for test\n",
        "]\n",
        "\n",
        "# List of non-Jerry audio samples for training and testing (Label 0)\n",
        "Non_Jerry_Audio_Files = [\n",
        "    \"L0-Sample01-Adolfo.wav\",  # Non-Jerry speaker\n",
        "    \"L0-Sample02-Rama.wav\",    # Non-Jerry speaker\n",
        "    \"L0-Sample03-Alex.wav\",    # Non-Jerry speaker\n",
        "    \"L0-Sample04-SynthGeorge.wav\",  # Synthetic voice\n",
        "    \"L0-Sample05-SynthJerry.wav\",   # Synthetic Jerry voice\n",
        "    \"L0-Sample06-SynthJerry.wav\",   # Synthetic Jerry voice\n",
        "    \"L0-Sample07-Teresa.wav\",  # Non-Jerry speaker\n",
        "    \"L0-Sample08-Blaine.wav\",  # Non-Jerry speaker\n",
        "    \"L0-Sample09-Bill.wav\",    # Non-Jerry speaker\n",
        "    \"L0-Sample10-Brian.wav\",   # Non-Jerry speaker\n",
        "    \"L0-Sample11-Chris.wav\",   # Non-Jerry speaker (test)\n",
        "    \"L0-Sample12-George.wav\",  # Non-Jerry speaker (test)\n",
        "]\n",
        "\n",
        "# Download a file from BASE_URL and save it to the current directory\n",
        "# if it does not already exist.\n",
        "def download_file(filename):\n",
        "\n",
        "    filepath = os.path.join(\"./\", filename)  # Local path in root\n",
        "    url = BASE_URL + filename\n",
        "    if not os.path.exists(filepath):  # Check if file exists\n",
        "        print(f\"Downloading {filename} to {filepath}...\")\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded {filename} successfully!\")\n",
        "        else:\n",
        "            print(f\"Failed to download {filename}. \"\n",
        "                  f\"Status code: {response.status_code}\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists at {filepath}.\")\n",
        "    return filepath  # Return the full path\n",
        "\n",
        "# Download files for Label 1 (Jerry's audio files)\n",
        "print(\"Processing Label 1 (Jerry's audio files)...\")\n",
        "for file in Jerry_Audio_Files:\n",
        "    download_file(file)\n",
        "\n",
        "# Download files for Label 0 (Non-Jerry audio files)\n",
        "print(\"\\nProcessing Label 0 (Non-Jerry audio files)...\")\n",
        "for file in Non_Jerry_Audio_Files:\n",
        "    download_file(file)\n",
        "\n",
        "print(\"\\nAll files are downloaded and ready!\")"
      ],
      "metadata": {
        "id": "nD-eSoAGRCbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 8-2: Audio Feature Extraction and Visualization\n",
        "\n",
        "The first cell defines extract_audio_features, which computes a compact audio\n",
        "fingerprint used for speaker recognition and deepfake detection.\n",
        "It summarizes timbre, brightness, bandwidth, and harmonic balance, along with\n",
        "a light anti-spoof cue—spectral flatness—to flag overly synthetic tones."
      ],
      "metadata": {
        "id": "xLQPXSjd4RoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "def extract_audio_features(file_path):\n",
        "    \"\"\"\n",
        "    Extract compact audio features for voice verification and light anti-spoof.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load mono at native sample rate\n",
        "    y, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "\n",
        "    # Simple voice-activity detection (drop silences for stable statistics)\n",
        "    intervals = librosa.effects.split(y, top_db=30)\n",
        "    if len(intervals):\n",
        "        y = np.concatenate([y[s:e] for s, e in intervals])\n",
        "\n",
        "    # Core spectral features\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    spec_roll = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    spec_con = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    rmse = librosa.feature.rms(y=y)\n",
        "\n",
        "    # Harmonic/percussive split for HNR proxy\n",
        "    harm = librosa.effects.harmonic(y)\n",
        "    perc = librosa.effects.percussive(y)\n",
        "    hnr = np.mean(harm) / (np.mean(perc) + 1e-6)\n",
        "\n",
        "    # Anti-spoof cue (higher ≈ more noise-like / synthetic)\n",
        "    flat = librosa.feature.spectral_flatness(y=y)\n",
        "\n",
        "    # Return dict; each key includes a short description\n",
        "    return {\n",
        "        \"mfcc_mean\": float(np.mean(mfccs)),                 # Avg MFCCs (timbre)\n",
        "        \"mfcc_std\": float(np.std(mfccs)),                   # MFCC variability\n",
        "        \"spectral_centroid_mean\": float(np.mean(spec_cent)),# Brightness center\n",
        "        \"spectral_rolloff_mean\": float(np.mean(spec_roll)), # High-end energy cut\n",
        "        \"spectral_bandwidth_mean\": float(np.mean(spec_bw)), # Bandwidth/spread\n",
        "        \"spectral_contrast_mean\": float(np.mean(spec_con)), # Peaks vs valleys\n",
        "        \"spectral_contrast_std\": float(np.std(spec_con)),   # Contrast variability\n",
        "        \"zcr_mean\": float(np.mean(zcr)),                    # Noisiness/frication\n",
        "        \"chroma_mean\": float(np.mean(chroma)),              # Pitch-class energy\n",
        "        \"hnr\": float(hnr),                                  # Harmonic-to-noise\n",
        "        \"rmse_mean\": float(np.mean(rmse)),                  # Loudness (RMS)\n",
        "        \"spectral_flatness_mean\": float(np.mean(flat)),     # Tonality→low, noise→high\n",
        "    }"
      ],
      "metadata": {
        "id": "Ensn6l4rkW20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cell 2 - Plot Basic Audio Features"
      ],
      "metadata": {
        "id": "j_xsQIHEFzwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "# Audio sample to plot basic features\n",
        "file_path = \"L1-Sample01-Jerry.wav\"\n",
        "\n",
        "# Download sample, if not done already\n",
        "download_file(file_path)\n",
        "\n",
        "# Extract features\n",
        "print(\"Extracting audio features...\")\n",
        "features = extract_audio_features(file_path)\n",
        "print(\"Extracted Features:\", features)\n",
        "\n",
        "# Load audio for visualization\n",
        "y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "# Plot MFCCs\n",
        "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mfccs, x_axis=\"time\", sr=sr, cmap=\"coolwarm\")\n",
        "plt.colorbar()\n",
        "plt.title(\"MFCCs\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Spectral Centroid\n",
        "spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(spectral_centroid[0], label=\"Spectral Centroid\")\n",
        "plt.legend()\n",
        "plt.title(\"Spectral Centroid\")\n",
        "plt.ylabel(\"Hz\")\n",
        "plt.xlabel(\"Frames\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Zero-Crossing Rate\n",
        "zcr = librosa.feature.zero_crossing_rate(y)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(zcr[0], label=\"Zero-Crossing Rate\")\n",
        "plt.legend()\n",
        "plt.title(\"Zero-Crossing Rate\")\n",
        "plt.xlabel(\"Frames\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fn7FPD2b3Cns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 8-3: Train Jerry Audio Detection Model\n",
        "This program trains a `logistic regression` model to distinguish Real Jerry audio from other voices. Each clip is converted into a compact fingerprint using MFCCs, spectral features, and harmonic cues, with silence removed for consistency. Features are standardized, then split into training and test sets. The model’s decision threshold is calibrated from the 95th percentile of Not Jerry scores to minimize false accepts. A lightweight anti-spoof rule adds a final check: if a “Real Jerry” prediction has a spectral flatness higher than Jerry’s natural range, it’s flipped to Not Real, catching clones without complicating the workflow."
      ],
      "metadata": {
        "id": "k59ge6YXsRxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Train/Test + threshold + flatness veto + single predict (globals) ===\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ----------------------------- Helpers --------------------------------\n",
        "def make_splits(jerry_list, non_list, holdout=2):\n",
        "    \"\"\"Return splits and labels; also return Jerry train files.\"\"\"\n",
        "    train_j = jerry_list[:-holdout]\n",
        "    test_j  = jerry_list[-holdout:]\n",
        "    train_n = non_list[:-holdout]\n",
        "    test_n  = non_list[-holdout:]\n",
        "    train_files = train_j + train_n\n",
        "    test_files  = test_j + test_n\n",
        "    y_train = np.array([1]*len(train_j) + [0]*len(train_n), dtype=int)\n",
        "    y_test  = np.array([1]*len(test_j)  + [0]*len(test_n), dtype=int)\n",
        "    return train_files, test_files, y_train, y_test, train_j\n",
        "\n",
        "def features_matrix(files):\n",
        "    \"\"\"Extract feature vectors (order = dict insertion from Cell 1).\"\"\"\n",
        "    vecs = []\n",
        "    for f in files:\n",
        "        feats = extract_audio_features(f)\n",
        "        vecs.append(np.array(list(feats.values()), np.float32))\n",
        "    return np.stack(vecs)\n",
        "\n",
        "def train_speaker(X_train, y_train):\n",
        "    \"\"\"Fit scaler + balanced logistic regression.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    Xn = scaler.fit_transform(X_train)\n",
        "    model = LogisticRegression(\n",
        "        class_weight=\"balanced\", max_iter=500, random_state=42\n",
        "    )\n",
        "    model.fit(Xn, y_train)\n",
        "    return scaler, model\n",
        "\n",
        "def neg_p95_threshold(model, scaler, X_train, y_train):\n",
        "    \"\"\"Cutoff from 95th percentile of negative (Not Jerry) scores.\"\"\"\n",
        "    Xn = scaler.transform(X_train)\n",
        "    scores = model.predict_proba(Xn)[:, 1]\n",
        "    neg = scores[y_train == 0]\n",
        "    thr = float(np.percentile(neg, 95)) if neg.size else 0.5\n",
        "    return thr\n",
        "\n",
        "def compute_flat_cap(train_jerry_files, pctl=99):\n",
        "    \"\"\"Cap for spectral_flatness_mean from Jerry-only training set.\"\"\"\n",
        "    vals = []\n",
        "    for f in train_jerry_files:\n",
        "        v = extract_audio_features(f)[\"spectral_flatness_mean\"]\n",
        "        vals.append(v)\n",
        "    vals = np.array(vals, dtype=np.float32)\n",
        "    cap = float(np.percentile(vals, pctl)) if vals.size else 1.0\n",
        "    return cap\n",
        "\n",
        "def evaluate_set(model, scaler, X_test, y_test, threshold):\n",
        "    \"\"\"Report accuracy and classification report.\"\"\"\n",
        "    Xn = scaler.transform(X_test)\n",
        "    scores = model.predict_proba(Xn)[:, 1]\n",
        "    y_pred = (scores >= threshold).astype(int)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(\"Test Accuracy:\", f\"{acc*100:.1f}%\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(\n",
        "        y_test, y_pred,\n",
        "        target_names=[\"Not Real Jerry\", \"Real Jerry\"],\n",
        "        zero_division=1\n",
        "    ))\n",
        "    return scores, y_pred\n",
        "\n",
        "# --------------------- Single-file predictor (global state) -----------\n",
        "def predict_new_wav(path_or_name):\n",
        "    \"\"\"\n",
        "    Predict Real/Not Real using the trained global pipeline.\n",
        "    Requires: voice_model, voice_scaler, voice_threshold, voice_flat_cap.\n",
        "    \"\"\"\n",
        "    missing = [\n",
        "        name for name in\n",
        "        [\"voice_model\", \"voice_scaler\", \"voice_threshold\", \"voice_flat_cap\"]\n",
        "        if name not in globals()\n",
        "    ]\n",
        "    if missing:\n",
        "        print(\"Model not initialized. Please rerun the training cell \"\n",
        "              \"to define: voice_model, voice_scaler, voice_threshold, \"\n",
        "              \"voice_flat_cap.\")\n",
        "        return None\n",
        "\n",
        "    path = path_or_name if os.path.exists(path_or_name) else f\"./{path_or_name}\"\n",
        "    feats = extract_audio_features(path)\n",
        "    x = np.array(list(feats.values()), np.float32).reshape(1, -1)\n",
        "    xn = voice_scaler.transform(x)\n",
        "    score = float(voice_model.predict_proba(xn)[0, 1])\n",
        "    pred = 1 if score >= voice_threshold else 0\n",
        "\n",
        "    flat = float(feats[\"spectral_flatness_mean\"])\n",
        "    if pred == 1 and flat > voice_flat_cap:\n",
        "        pred = 0  # veto to Not Real\n",
        "\n",
        "    label = \"Real Jerry\" if pred == 1 else \"Not Real Jerry\"\n",
        "    print(f\"\\nNew file: {os.path.basename(path)} | Score: {score:.3f} | \"\n",
        "          f\"Flatness: {flat:.3f} (cap {voice_flat_cap:.3f}) | \"\n",
        "          f\"Thr: {voice_threshold:.2f} | Pred: {label}\")\n",
        "    return pred, score\n",
        "\n",
        "# --------------------------- Main control -----------------------------\n",
        "# 1) Build splits\n",
        "train_files, test_files, y_train, y_test, train_jerry_files = make_splits(\n",
        "    Jerry_Audio_Files, Non_Jerry_Audio_Files, holdout=2\n",
        ")\n",
        "print(f\"Total training files: {len(train_files)}\")\n",
        "print(f\"Total test files: {len(test_files)}\")\n",
        "\n",
        "# 2) Feature matrices\n",
        "X_train = features_matrix(train_files)\n",
        "X_test  = features_matrix(test_files)\n",
        "print(\"Extracted training features shape:\", X_train.shape)\n",
        "print(\"Extracted test features shape:\", X_test.shape)\n",
        "\n",
        "# 3) Train speaker model  → store as descriptive globals\n",
        "voice_scaler, voice_model = train_speaker(X_train, y_train)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# 4) Decision threshold from negatives (P95)  → global\n",
        "voice_threshold = neg_p95_threshold(voice_model, voice_scaler, X_train, y_train)\n",
        "print(f\"Decision threshold from negatives (P95): {voice_threshold:.2f}\")\n",
        "\n",
        "# 5) Jerry-only flatness cap (P99) for minimal anti-spoof veto  → global\n",
        "voice_flat_cap = compute_flat_cap(train_jerry_files, pctl=99)\n",
        "print(f\"Jerry flatness cap (P99): {voice_flat_cap:.3f}\")\n",
        "\n",
        "# 6) Evaluate on held-out set\n",
        "scores, y_pred = evaluate_set(\n",
        "    voice_model, voice_scaler, X_test, y_test, voice_threshold\n",
        ")\n",
        "\n",
        "# 7) Per-file predictions\n",
        "for f, t, s, yhat in zip(test_files, y_test, scores, y_pred):\n",
        "    t_str = \"Real Jerry\" if t == 1 else \"Not Real Jerry\"\n",
        "    y_str = \"Real Jerry\" if yhat == 1 else \"Not Real Jerry\"\n",
        "    print(f\"File: {os.path.basename(f):24s} Score: {s:.3f} \"\n",
        "          f\"True: {t_str:13s} Pred: {y_str}\")"
      ],
      "metadata": {
        "id": "Ni7AG4wNnDvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Acid test: two cloned clips then one authentic control ---\n",
        "\n",
        "# Test 1: cloned sample (should be flagged as Not Real Jerry)\n",
        "download_file(\"Jerry-Cloned-Sample01.wav\")\n",
        "_ = predict_new_wav(\"Jerry-Cloned-Sample01.wav\")  # uses global voice_* vars\n",
        "\n",
        "# Test 2: another cloned sample to confirm consistent detection\n",
        "download_file(\"Jerry-Cloned-Sample02.wav\")\n",
        "_ = predict_new_wav(\"Jerry-Cloned-Sample02.wav\")  # uses global voice_* vars\n",
        "\n",
        "# Control: a real Jerry clip to verify authentic detection still passes\n",
        "download_file(\"L1-Sample01-Jerry.wav\")\n",
        "_ = predict_new_wav(\"L1-Sample01-Jerry.wav\")      # uses global voice_* vars)"
      ],
      "metadata": {
        "id": "qu9L3bWfXYfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 8-4: Transcribe Jerry's Real Audio to Text\n",
        "This program downloads Real Jerry audio files, transcribes them using OpenAI's Whisper model, and saves the results in a dictionary for further use in other programs."
      ],
      "metadata": {
        "id": "0c_S4i4AyzhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\n",
        "import librosa\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Function to transcribe audio files using Whisper model\n",
        "def transcribe_audio_files(file_list, output_csv=\"transcriptions.csv\"):\n",
        "    \"\"\"\n",
        "    Transcribe audio files using Whisper model and save the filename and\n",
        "    transcription to a CSV file.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Load Whisper model and processor\n",
        "    print(\"Loading Whisper model...\")\n",
        "    try:\n",
        "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "        model = WhisperForConditionalGeneration.from_pretrained(\n",
        "            \"openai/whisper-small\"\n",
        "        )\n",
        "        model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Whisper model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Whisper model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Process each file\n",
        "    for file in file_list:\n",
        "        print(f\"Processing {file}...\")\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess audio\n",
        "            audio, sr = librosa.load(file, sr=16000)  # Ensure 16 kHz sampling rate\n",
        "            inputs = processor(\n",
        "                audio, sampling_rate=16000, return_tensors=\"pt\", language=\"en\"\n",
        "            ).input_features\n",
        "            inputs = inputs.to(model.device)\n",
        "\n",
        "            # Transcribe the audio\n",
        "            predicted_ids = model.generate(inputs)\n",
        "            transcription = processor.batch_decode(\n",
        "                predicted_ids, skip_special_tokens=True\n",
        "            )[0]\n",
        "\n",
        "            results.append({\"filename\": file, \"transcription\": transcription})\n",
        "            print(f\"Transcription for {file}: {transcription}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {file}: {e}\")\n",
        "            results.append({\"filename\": file, \"transcription\": None})\n",
        "\n",
        "    # Save results to CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Transcriptions saved to {output_csv}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Transcribe Real Jerry files\n",
        "real_jerry_transcriptions = transcribe_audio_files(Jerry_Audio_Files)\n",
        "\n",
        "# Print transcriptions\n",
        "if real_jerry_transcriptions:\n",
        "    print(\"\\n--- Transcriptions ---\")\n",
        "    for entry in real_jerry_transcriptions:\n",
        "        print(f\"{entry['filename']}: {entry['transcription']}\")\n",
        "else:\n",
        "    print(\"No transcriptions available due to an error.\")"
      ],
      "metadata": {
        "id": "0W0Hv2qxy3hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 8-5: Voice Cloning Listings\n",
        "\n",
        "This section\n",
        "It includes the following steps:\n",
        "\n",
        "1. **Step 1:** Installs the necessary libraries and checks for GPU availability.\n",
        "2. **Step 2:** Dataset Preparation and Embedding.\n",
        "3. **Step 3:** Fine-Tuning the SpeehT5 Model.\n",
        "4. **Step 4:** Testing Jerry's cloned voice. How does it sound?\n",
        "5. **Step 5:** Comparing Feature Differences: Real vs. Cloned"
      ],
      "metadata": {
        "id": "YmQ_QxFWp4bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Prerequisite Setup\n",
        "This section installs the necessary libraries and checks for GPU availability to prepare the environment for using SpeechT5 and HiFi-GAN for text-to-speech synthesis.\n",
        "\n",
        "It includes the following steps:\n",
        "\n",
        "1. **Install Libraries:** Installs `datasets`, `soundfile`, `speechbrain`, `transformers`, and `accelerate` using `pip`.\n",
        "2. **Check GPU:** Verifies the availability of a GPU using `nvidia-smi`.\n",
        "3. **Import Libraries:** Imports the required modules from `transformers` and `torch`.\n",
        "4. **Load Models:** Loads the SpeechT5 processor, model, and HiFi-GAN vocoder.\n",
        "5. **Device Setup:** Checks for GPU availability and moves the model and vocoder to the appropriate device (GPU or CPU).\n",
        "\n",
        "**Note:** Colab ships with TensorFlow and Keras 3 preinstalled, which can confuse\n",
        "🤗 Transformers into loading the wrong backend. To avoid this, we disable\n",
        "TensorFlow/Flax, install a small tf-keras shim, and sometimes restart the\n",
        "runtime so changes take effect. If you later see errors about Keras 3 or\n",
        "tf_keras, rerun the install cell, restart the runtime, or upgrade\n",
        "Transformers. Future versions will likely remove the need for this workaround."
      ],
      "metadata": {
        "id": "_hA1YPRC5ZSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: Colab (GPU optional), python>=3.9\n",
        "# Run after a fresh runtime restart. Do NOT import transformers in this cell.\n",
        "\n",
        "# Keep Transformers on PyTorch only (disable TF/Flax backends)\n",
        "%env TRANSFORMERS_NO_TF=1\n",
        "%env TRANSFORMERS_NO_FLAX=1\n",
        "%env USE_TF=0\n",
        "%env USE_FLAX=0\n",
        "%env TOKENIZERS_PARALLELISM=false\n",
        "\n",
        "# Clean install; ignore unrelated resolver warnings. No transformers import.\n",
        "!pip -q uninstall -y torch torchaudio torchvision >/dev/null 2>&1\n",
        "!pip -q install torch==2.8.0 torchaudio==2.8.0 torchvision==0.23.0 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu126\n",
        "# CPU alt:\n",
        "# !pip -q install torch==2.8.0+cpu torchaudio==2.8.0+cpu \\\n",
        "#   torchvision==0.23.0+cpu --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "!pip -q install tf-keras==2.15.0\n",
        "!pip -q install --force-reinstall pyarrow==16.1.0\n",
        "!pip -q install -U datasets==2.20.0 \"fsspec>=2024.5.0,<2025.1\"\n",
        "!pip -q install -U transformers accelerate speechbrain soundfile librosa \\\n",
        "  peft sentencepiece\n",
        "\n",
        "# Quick check without importing transformers\n",
        "import torch, pyarrow, datasets, sys\n",
        "print(\"torch:\", torch.__version__, \"cuda?\", torch.cuda.is_available())\n",
        "print(\"pyarrow:\", pyarrow.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "\n",
        "print(\"\\n\" + \"=\"*72)\n",
        "print(\"NEXT STEP REQUIRED:\")\n",
        "print(\"1) Go to: Runtime → Restart Session (IMPORTANT).\")\n",
        "print(\"2) After restart, run Step 1 (model load) and continue.\")\n",
        "print(\"=\"*72)\n",
        "\n",
        "# Extra safety: warn if transformers got imported earlier by mistake\n",
        "if \"transformers\" in sys.modules:\n",
        "    print(\"\\nWARNING: 'transformers' is already imported in this session.\")\n",
        "    print(\"Please Runtime → Restart runtime, then re-run Step 1.\")"
      ],
      "metadata": {
        "id": "HicrnDMhDOQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: python>=3.9, torch, transformers, sentencepiece; GPU optional\n",
        "\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
        "from transformers import SpeechT5HifiGan\n",
        "import torch\n",
        "\n",
        "# Load processor and base model (text to acoustic features)\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "\n",
        "# Disable caching to support gradient checkpointing if used later\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Load HiFi-GAN vocoder to convert features to waveform\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "# Use GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "vocoder.to(device)\n",
        "\n",
        "print(f\"Models ready on {device}\")"
      ],
      "metadata": {
        "id": "CFQJ36ZjeZej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Dataset Preparation and Embedding\n",
        "This program processes audio files and their transcripts to create a dataset for voice cloning. It integrates audio features, transcriptions, and speaker embeddings into a Hugging Face Dataset, ready for training or testing voice cloning models. Long samples are filtered, and the dataset is split into train-test subsets."
      ],
      "metadata": {
        "id": "_EhfJKxGsJHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Helper functions for Step 2 — keep hidden in book)\n",
        "\n",
        "import os, numpy as np, pandas as pd, torch\n",
        "from datasets import Dataset, Audio\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "def load_csv_validate(csv_path: str):\n",
        "    # This function loads the CSV and checks required columns\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    need = {\"filename\", \"transcription\"}\n",
        "    if not need.issubset(df.columns):\n",
        "        raise ValueError(\"CSV must have 'filename' and 'transcription'.\")\n",
        "    return df\n",
        "\n",
        "def make_paths(df, audio_dir: str):\n",
        "    # This function builds audio paths and returns texts\n",
        "    file_paths = [os.path.join(audio_dir, f) for f in df[\"filename\"]]\n",
        "    missing = [p for p in file_paths if not os.path.exists(p)]\n",
        "    if missing:\n",
        "        raise FileNotFoundError(f\"Missing audio; first few: {missing[:5]}\")\n",
        "    texts = df[\"transcription\"].tolist()\n",
        "    return file_paths, texts\n",
        "\n",
        "def build_hf_dataset(file_paths, texts, sampling_rate: int):\n",
        "    # This function builds a HF dataset and casts audio with a given rate\n",
        "    ds = Dataset.from_dict({\"file_path\": file_paths, \"text\": texts})\n",
        "    return ds.cast_column(\"file_path\", Audio(sampling_rate=sampling_rate))\n",
        "\n",
        "def load_speaker_model(device: str, savedir: str):\n",
        "    # This function loads the SpeechBrain x-vector encoder\n",
        "    name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "    return EncoderClassifier.from_hparams(\n",
        "        source=name, run_opts={\"device\": device}, savedir=savedir\n",
        "    )\n",
        "\n",
        "def normalize_labels(arr: np.ndarray, mel_bins: int) -> np.ndarray:\n",
        "    # This function forces labels to shape (T, mel_bins)\n",
        "    x = np.asarray(arr, dtype=np.float32)\n",
        "    x = np.squeeze(x)                 # handles (1, T, 80) etc.\n",
        "    if x.ndim == 1:\n",
        "        x = x[:, None]                # (T,) -> (T,1)\n",
        "    if x.ndim != 2:\n",
        "        raise ValueError(f\"labels must be 2D after squeeze, got {x.shape}\")\n",
        "    if x.shape[-1] == mel_bins:\n",
        "        return x                      # (T, 80)\n",
        "    if x.shape[0] == mel_bins:\n",
        "        return x.T                    # (80, T) -> (T, 80)\n",
        "    if mel_bins in x.shape and x.shape[-1] != mel_bins:\n",
        "        axis = 0 if x.shape[0] == mel_bins else 1\n",
        "        x = np.swapaxes(x, axis, 1)\n",
        "    if x.shape[-1] != mel_bins:\n",
        "        raise ValueError(f\"labels not (T,{mel_bins}); got {x.shape}\")\n",
        "    return x\n",
        "\n",
        "def prepare_example(ex, processor, spk_model, device, mel_bins: int):\n",
        "    # This function tokenizes text, makes labels, and adds embeddings\n",
        "    a = ex[\"file_path\"]               # {'array', 'sampling_rate'}\n",
        "    proc = processor(\n",
        "        text=ex[\"text\"],\n",
        "        audio_target=a[\"array\"],\n",
        "        sampling_rate=a[\"sampling_rate\"],\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "    labels = normalize_labels(proc[\"labels\"], mel_bins)\n",
        "    with torch.no_grad():\n",
        "        emb = spk_model.encode_batch(\n",
        "            torch.tensor(a[\"array\"]).unsqueeze(0).to(device)\n",
        "        ).squeeze().detach().cpu().numpy().astype(np.float32)\n",
        "    return {\n",
        "        \"input_ids\": proc[\"input_ids\"],\n",
        "        \"attention_mask\": proc[\"attention_mask\"],\n",
        "        \"labels\": labels,\n",
        "        \"speaker_embeddings\": emb,\n",
        "        \"text\": ex[\"text\"],\n",
        "    }\n",
        "\n",
        "def filter_long_texts(ex, max_tokens=200):\n",
        "    # This function filters out long texts by token length\n",
        "    return len(ex[\"input_ids\"]) < max_tokens\n",
        "\n",
        "def print_alignment_sample(ds, sr: int, mel_bins: int):\n",
        "    # This function prints shapes so Step 3 expectations are clear\n",
        "    import numpy as _np\n",
        "    ex0 = ds[0]\n",
        "    print(\"\\n=== Alignment assumptions for Step 3 ===\")\n",
        "    print(f\"- sampling_rate: {sr}\")\n",
        "    print(f\"- mel_bins (labels last dim): {mel_bins}\")\n",
        "    print(f\"- input_ids len: {len(ex0['input_ids'])}\")\n",
        "    print(f\"- attention_mask len: {len(ex0['attention_mask'])}\")\n",
        "    print(f\"- labels shape: {_np.asarray(ex0['labels']).shape}\")\n",
        "    print(f\"- speaker_embeddings shape: \"\n",
        "          f\"{_np.asarray(ex0['speaker_embeddings']).shape}\")\n",
        "    print(\"Step 3 pads text to batch max, labels on T with -100, \"\n",
        "          \"and batches speaker embeddings as float32.\")"
      ],
      "metadata": {
        "id": "rTIWgAMPsLic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: processor from Step 1; datasets, speechbrain, numpy, torch\n",
        "#           'transcriptions.csv' with filename, transcription; audio at 16 kHz\n",
        "\n",
        "import os, torch\n",
        "\n",
        "print(\"Step 2: Starting data preparation...\")\n",
        "\n",
        "# Assumptions that Step 3 depends on\n",
        "TARGET_SR = 16000          # audio sampling rate\n",
        "MEL_BINS  = 80             # labels have 80 mel bins (shape = T, 80)\n",
        "\n",
        "# Set paths for CSV, audio, and output dataset\n",
        "root_dir    = \"./\"\n",
        "audio_dir   = root_dir\n",
        "csv_path    = \"transcriptions.csv\"\n",
        "dataset_dir = os.path.join(root_dir, \"processed_dataset\")\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV and build a Hugging Face dataset with audio casting\n",
        "df = load_csv_validate(csv_path)\n",
        "paths, texts = make_paths(df, audio_dir)\n",
        "ds = build_hf_dataset(paths, texts, TARGET_SR)\n",
        "\n",
        "# Pick device and load the speaker embedding model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "spk = load_speaker_model(device, savedir=\"/tmp/spk_xvect_voxceleb\")\n",
        "\n",
        "# Preparing examples: tokenize, build labels, and add embeddings\n",
        "def _map_fn(ex):\n",
        "    return prepare_example(\n",
        "        ex, processor=processor, spk_model=spk,\n",
        "        device=device, mel_bins=MEL_BINS\n",
        "    )\n",
        "\n",
        "# Tokenize audio+text and add speaker embeddings.\n",
        "# - _map_fn: builds input_ids, attention_mask, labels, embeddings.\n",
        "# - remove_columns: drop raw \"file_path\" after we extract audio.\n",
        "# - num_proc=1: keep it single-process in Colab to avoid quirks.\n",
        "print(\"[info] tokenizing and computing speaker embeddings...\")\n",
        "ds = ds.map(_map_fn, remove_columns=[\"file_path\"], num_proc=1)\n",
        "\n",
        "# Filter out long texts so batches stay small and training is stable.\n",
        "# - filter_long_texts: keeps examples with <200 tokens (you can tune).\n",
        "print(\"[info] filtering long texts...\")\n",
        "ds = ds.filter(filter_long_texts)\n",
        "\n",
        "# Print one example so readers can see shapes that Step 3 expects.\n",
        "# - sr: target sampling rate used when casting audio (16 kHz here).\n",
        "# - mel_bins: number of mel features per frame (80 here).\n",
        "print_alignment_sample(ds, sr=TARGET_SR, mel_bins=MEL_BINS)\n",
        "\n",
        "# Create a train/test split, then persist to disk for Step 3.\n",
        "# - test_size=0.1: 10% of data goes to the test set.\n",
        "# - seed=42: fixed split for reproducibility.\n",
        "print(\"\\n[info] splitting train/test and saving to disk...\")\n",
        "ds = ds.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "# - save_to_disk: saves to 'processed_dataset' directory.\n",
        "ds.save_to_disk(dataset_dir)\n",
        "print(\"[done] Data preparation complete.\")"
      ],
      "metadata": {
        "id": "BabXeavfV3W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Fine-Tuning the Model\n",
        "This program fine-tunes a SpeechT5 model for text-to-speech conversion using a processed dataset. It includes a custom data collator for speaker embeddings, trains the model with Hugging Face's Seq2SeqTrainer, and saves the fine-tuned model and processor to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "S2xy21k4t3QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Helper(s) for Step 3 — keep hidden in book)\n",
        "\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "def ensure_even_T(x: torch.Tensor) -> torch.Tensor:\n",
        "    # This function trims last frame if T is odd so T is even\n",
        "    return x[:-1, :] if (x.shape[0] % 2 == 1) else x\n",
        "\n",
        "@dataclass\n",
        "class TTSDataCollator:\n",
        "    # This class pads text, pads labels on time, batches embeddings\n",
        "    pad_id: int = 0\n",
        "    mel_bins: int = 80\n",
        "\n",
        "    def __call__(self, feats: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        # Pad text inputs (ids, attention) to batch max length\n",
        "        ids  = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in feats]\n",
        "        amsk = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in feats]\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            ids, batch_first=True, padding_value=self.pad_id\n",
        "        )\n",
        "        attention = torch.nn.utils.rnn.pad_sequence(\n",
        "            amsk, batch_first=True, padding_value=0\n",
        "        )\n",
        "\n",
        "        # Make label lengths even, then pad on time with -100\n",
        "        labs = [torch.tensor(f[\"labels\"], dtype=torch.float32) for f in feats]\n",
        "        labs = [ensure_even_T(x) for x in labs]  # each x is (T, mel_bins)\n",
        "        t_max = max(x.shape[0] for x in labs)\n",
        "        labels = torch.full(\n",
        "            (len(labs), t_max, self.mel_bins), -100.0, dtype=torch.float32\n",
        "        )\n",
        "        for i, x in enumerate(labs):\n",
        "            t = x.shape[0]\n",
        "            labels[i, :t, :] = x\n",
        "\n",
        "        # Stack speaker embeddings to a batch\n",
        "        spk = torch.stack(\n",
        "            [torch.tensor(f[\"speaker_embeddings\"], dtype=torch.float32)\n",
        "             for f in feats],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention,\n",
        "            \"labels\": labels,\n",
        "            \"speaker_embeddings\": spk,\n",
        "        }"
      ],
      "metadata": {
        "id": "I6OfFGgSt63n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: processor+model from Step 1; Step 2 saved dataset on disk\n",
        "#           transformers, datasets, torch; GPU optional\n",
        "\n",
        "from datasets import load_from_disk\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch, os\n",
        "\n",
        "print(\"Step 3: Starting fine-tuning process...\")\n",
        "\n",
        "# Load the dataset prepared in Step 2\n",
        "dataset = load_from_disk(\"./processed_dataset\")\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Build the collator (pads text, pads labels on time, batches spk)\n",
        "pad_id = getattr(getattr(processor, \"tokenizer\", None), \"pad_token_id\", 0)\n",
        "data_collator = TTSDataCollator(pad_id=pad_id, mel_bins=80)\n",
        "\n",
        "# Optional: Hugging Face Hub token (for push_to_hub)\n",
        "hf_token = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get(\"HF_TOKEN\")\n",
        "except Exception:\n",
        "    hf_token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "# Configure training (quick tuning tips):\n",
        "# - Start small: batch=2, accum=8; raise if memory allows.\n",
        "# - If loss is noisy, lower LR (e.g., 5e-5) or raise warmup steps.\n",
        "# - If training is slow, cut max_steps or save less often.\n",
        "# - Enable fp16 on GPU to save memory; disable if you see NaNs.\n",
        "# - Keep remove_unused_columns=False so speaker embeddings stay in.\n",
        "# - Set label_names to \"labels\" so masking works as expected.\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./speecht5_finetuned_model\",   # save checkpoints here\n",
        "    per_device_train_batch_size=2,             # batch per GPU/CPU; lower if OOM\n",
        "    gradient_accumulation_steps=8,             # virtual batch = 2*8 per update\n",
        "    learning_rate=1e-4,                        # optimizer step size\n",
        "    warmup_steps=200,                          # steps to ramp LR from 0\n",
        "    max_steps=2000,                            # total training steps\n",
        "    fp16=torch.cuda.is_available(),            # use half precision on GPU\n",
        "    logging_steps=100,                         # log every N steps\n",
        "    save_steps=500,                            # checkpoint every N steps\n",
        "    report_to=[],                              # disable wandb/tensorboard\n",
        "    push_to_hub=bool(hf_token),                # upload to HF Hub if token set\n",
        "    hub_token=hf_token,                        # HF token (or None)\n",
        "    remove_unused_columns=False,               # keep speaker_embeddings\n",
        "    label_names=[\"labels\"],                    # tell Trainer our label field\n",
        "    dataloader_num_workers=0,                  # avoid Colab multi-proc issues\n",
        ")\n",
        "\n",
        "# Create the Trainer\n",
        "# - processing_class: lets Trainer handle text post-proc via processor\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                       # from Step 1\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=processor,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save artifacts for Step 4 (and optionally push to Hub)\n",
        "# print(\"Saving fine-tuned model and processor...\")\n",
        "# trainer.save_model(\"./speecht5_finetuned_model\")\n",
        "# processor.save_pretrained(\"./speecht5_finetuned_model\")\n",
        "# if hf_token:\n",
        "#    trainer.push_to_hub()\n",
        "\n",
        "print(\"Fine-tuning process complete!\")"
      ],
      "metadata": {
        "id": "mApamguWmlim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Testing and Play Synthesized Speech\n",
        "This program tests the fine-tuned SpeechT5 model by generating a spectrogram and converting it to audio using a vocoder. It synthesizes speech from custom text input using the selected speaker embedding, visualizes the spectrogram, and saves the generated audio file for playback."
      ],
      "metadata": {
        "id": "o1QcwZybwCTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: Step 3 outputs in ./speecht5_finetuned_model; Step 2 dataset;\n",
        "#           torch, transformers, soundfile; GPU optional.\n",
        "\n",
        "import torch, soundfile as sf\n",
        "from IPython.display import Audio\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        ")\n",
        "\n",
        "print(\"Step 4: Testing the fine-tuned model...\")\n",
        "\n",
        "# Load fine-tuned artifacts (processor, model) and the vocoder\n",
        "model_dir = \"./speecht5_finetuned_model\"\n",
        "processor = SpeechT5Processor.from_pretrained(model_dir)\n",
        "model     = SpeechT5ForTextToSpeech.from_pretrained(model_dir)\n",
        "vocoder   = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "# Put model and vocoder on GPU if available (faster), else CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device); vocoder.to(device)\n",
        "\n",
        "# Load a speaker embedding from Step 2's saved dataset (test split)\n",
        "dataset = load_from_disk(\"./processed_dataset\")\n",
        "ex = dataset[\"test\"][0]\n",
        "speaker_embeddings = torch.tensor(\n",
        "    ex[\"speaker_embeddings\"], dtype=torch.float32\n",
        ").unsqueeze(0).to(device)\n",
        "\n",
        "# Text to synthesize (try a sentence the speaker would plausibly say)\n",
        "text = (\"Hey ladies and gentlemen, thank you for tuning in to the \"\n",
        "        \"Wild Ducks podcast featuring your host, Jerry Cuomo.\")\n",
        "\n",
        "# Tokenize text and move tensors to the same device as the model\n",
        "inputs = processor(text=text, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate waveform directly; SpeechT5 calls HiFi-GAN under the hood\n",
        "with torch.no_grad():\n",
        "    speech = model.generate_speech(\n",
        "        inputs[\"input_ids\"],                 # tokenized text\n",
        "        speaker_embeddings=speaker_embeddings,  # voice identity\n",
        "        vocoder=vocoder                     # waveform generator\n",
        "    )  # returns a 1D waveform tensor\n",
        "\n",
        "# --- Generate + SAVE the cloned audio ---\n",
        "clone_wav = \"Jerry-Cloned-Sample01.wav\"\n",
        "sf.write(clone_wav, speech.cpu().numpy(), 16000)  # save the new clone\n",
        "print(\"Wrote cloned WAV:\", clone_wav)\n",
        "\n",
        "# Sanity: file sizes and durations should differ if content differs\n",
        "import os, soundfile as sf, numpy as np\n",
        "real_wav = \"L1-Sample11-Jerry.wav\"\n",
        "for p in [real_wav, clone_wav]:\n",
        "    y, sr = sf.read(p)\n",
        "    print(f\"{p}: sr={sr}, dur={len(y)/sr:.2f}s, bytes={os.path.getsize(p)}\")\n",
        "\n",
        "# --- Side-by-side embedded players (shows proper durations) ---\n",
        "from IPython.display import Audio, HTML, display\n",
        "a_real  = Audio(filename=real_wav,  rate=16000, embed=True)\n",
        "a_clone = Audio(filename=clone_wav, rate=16000, embed=True)\n",
        "\n",
        "html = f\"\"\"\n",
        "<div style=\"display:flex;gap:24px;align-items:flex-start;\">\n",
        "  <div><div style=\"font:600 14px system-ui;margin-bottom:6px;\">Real</div>\n",
        "    {a_real._repr_html_()}\n",
        "  </div>\n",
        "  <div><div style=\"font:600 14px system-ui;margin-bottom:6px;\">Cloned</div>\n",
        "    {a_clone._repr_html_()}\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html))"
      ],
      "metadata": {
        "id": "JXjhcegDwEcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 - Comparing Real Audio versus Memorex (Cloned)\n",
        "\n",
        "This program extracts audio features from real and cloned samples, computes their differences, and visualizes them in a bar chart. The plot highlights subtle variations in the audio fingerprints, helping identify key features where synthetic audio deviates from real recordings.\n"
      ],
      "metadata": {
        "id": "3LGpRX3j-SGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REQUIRES: librosa, pandas, matplotlib\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract audio features remains the same as provided above\n",
        "\n",
        "# Paths to real and cloned audio samples\n",
        "real_audio_path = \"L1-Sample11-Jerry.wav\"\n",
        "cloned_audio01_path = \"Jerry-Cloned-Sample01.wav\"\n",
        "cloned_audio02_path = \"Jerry-Cloned-Sample02.wav\"\n",
        "\n",
        "# Extract features for real and cloned audio\n",
        "\n",
        "# --- Acid test: two cloned clips then one authentic control ---\n",
        "\n",
        "# Test 1: cloned sample (should be flagged as Not Real Jerry)\n",
        "download_file(cloned_audio01_path)\n",
        "_ = predict_new_wav(cloned_audio01_path)  # uses global voice_* vars\n",
        "\n",
        "# Test 2: another cloned sample to confirm consistent detection\n",
        "download_file(cloned_audio02_path)\n",
        "_ = predict_new_wav(cloned_audio02_path)  # uses global voice_* vars\n",
        "\n",
        "# Control: a real Jerry clip to verify authentic detection still passes\n",
        "download_file(real_audio_path)\n",
        "_ = predict_new_wav(real_audio_path)      # uses global voice_* vars)\n",
        "\n",
        "\n",
        "# Extract features for real and cloned audio\n",
        "real_features = extract_audio_features(real_audio_path)\n",
        "cloned_features = extract_audio_features(cloned_audio01_path)\n",
        "\n",
        "# Create a DataFrame for easy comparison\n",
        "feature_df = pd.DataFrame([real_features, cloned_features], index=[\"Real\", \"Cloned\"])\n",
        "\n",
        "# Normalize features for comparison (min-max scaling)\n",
        "normalized_feature_df = (feature_df - feature_df.min()) / (feature_df.max() - feature_df.min())\n",
        "\n",
        "# Plot feature comparison\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute feature-wise differences\n",
        "feature_differences = feature_df.loc[\"Real\"] - feature_df.loc[\"Cloned\"]\n",
        "\n",
        "# Plot the feature differences\n",
        "plt.figure(figsize=(12, 6))\n",
        "feature_differences.plot(kind=\"bar\", color=\"red\", edgecolor=\"black\")\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title(\"Feature-wise Differences: Real vs. Cloned Audio\")\n",
        "plt.ylabel(\"Difference in Feature Value\")\n",
        "plt.xlabel(\"Audio Features\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sg4JZURxJn9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Analysis"
      ],
      "metadata": {
        "id": "UaLU7B7ZHkW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 8-6: Scene Detection with OpenCV\n",
        "Detects video scenes using `OpenCV`. Outputs the scene number, with start and start times, and scene duration.\n",
        "\n",
        "** Note:** This program attempts to download a sample video file from this books Github. If you get an error downloading, simply rerun the first code cell in this notebook that defines the `download_file` function."
      ],
      "metadata": {
        "id": "qH3iKkEZiuaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install opencv-python numpy torch torchvision scenedetect yolov5"
      ],
      "metadata": {
        "id": "SIzcX-YPcXSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scenedetect import SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from scenedetect.backends.opencv import VideoCaptureAdapter\n",
        "import cv2  # For OpenCV VideoCapture\n",
        "\n",
        "# Input video file path\n",
        "INPUT_VIDEO_FILE = \"Jerry-Jose-SampleVideo01.mp4\"\n",
        "download_file(INPUT_VIDEO_FILE)  # Ensure the file is downloaded\n",
        "\n",
        "# Check if the input video file exists\n",
        "if not os.path.exists(INPUT_VIDEO_FILE):\n",
        "    print(f\"Error: File '{INPUT_VIDEO_FILE}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# Initialize OpenCV VideoCapture for reading the video\n",
        "video_capture = cv2.VideoCapture(INPUT_VIDEO_FILE)\n",
        "if not video_capture.isOpened():\n",
        "    print(f\"Error: Unable to open video file '{INPUT_VIDEO_FILE}'.\")\n",
        "    exit()\n",
        "\n",
        "# Create a VideoCaptureAdapter for SceneDetect compatibility\n",
        "video_adapter = VideoCaptureAdapter(video_capture)\n",
        "\n",
        "# Initialize SceneManager for scene detection\n",
        "scene_manager = SceneManager()\n",
        "\n",
        "# Add a ContentDetector to detect scene transitions\n",
        "# Lower threshold (e.g., 8.0) = more sensitive to changes\n",
        "scene_manager.add_detector(ContentDetector(threshold=8.0))\n",
        "\n",
        "# Perform scene detection on the video\n",
        "scene_manager.detect_scenes(video_adapter)\n",
        "\n",
        "# Retrieve the list of detected scenes with start and end times\n",
        "scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "# Filter scenes to exclude those shorter than 4 seconds\n",
        "filtered_scene_list = [\n",
        "    (start, end)\n",
        "    for start, end in scene_list\n",
        "    if (end - start).get_seconds() >= 4  # Minimum scene length filter\n",
        "]\n",
        "\n",
        "# Output the number of detected scenes\n",
        "print(f\"Detected {len(filtered_scene_list)} scenes.\")\n",
        "\n",
        "# Print details of each filtered scene\n",
        "for i, (start_time, end_time) in enumerate(filtered_scene_list):\n",
        "    print(f\"Scene {i + 1}: Start - {start_time}, End - {end_time}\")\n",
        "\n",
        "# Release the video capture resource\n",
        "video_capture.release()"
      ],
      "metadata": {
        "id": "VRZbu9_n1J6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 8-7: Video Object Detection and Annotation\n",
        "\n",
        "The code demonstrates using `YOLOv5` for real-time object detection on video frames. It processes each frame, detects objects, annotates with bounding boxes and labels, and saves the output video.\n"
      ],
      "metadata": {
        "id": "tyTeuGKQcIRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings, such as FutureWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Load the pre-trained YOLOv5 model for object detection\n",
        "# 'yolov5s' is a small, pre-trained YOLOv5 model optimized for speed\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Define input video file and output video file paths\n",
        "INPUT_VIDEO_FILE = \"Jerry-Jose-SampleVideo01.mp4\"  # Input video to process\n",
        "OUTPUT_VIDEO_FILE = \"Jerry-Jose-SampleVideo02.mp4\"  # Annotated output video\n",
        "\n",
        "# Ensure the input video file is downloaded or exists\n",
        "download_file(INPUT_VIDEO_FILE)\n",
        "\n",
        "# Load the input video using OpenCV\n",
        "cap = cv2.VideoCapture(INPUT_VIDEO_FILE)\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Unable to open video file '{INPUT_VIDEO_FILE}'.\")\n",
        "    exit()\n",
        "\n",
        "# Get video properties: width, height, and frames per second (FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Initialize the video writer to save the output video with annotations\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_FILE,\n",
        "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                      fps, (width, height))\n",
        "\n",
        "# Process the video frame by frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()  # Read the next frame\n",
        "    if not ret:  # If no more frames are available, exit the loop\n",
        "        break\n",
        "\n",
        "    # Convert the frame to RGB format (required by YOLOv5)\n",
        "    results = model(frame)  # Perform object detection on the frame\n",
        "\n",
        "    # Get detection results as a pandas DataFrame\n",
        "    detected_objects = results.pandas().xyxy[0]\n",
        "\n",
        "    # Annotate the frame with bounding boxes and labels\n",
        "    for _, row in detected_objects.iterrows():\n",
        "        # Extract bounding box coordinates and object details\n",
        "        x1, y1 = int(row['xmin']), int(row['ymin'])\n",
        "        x2, y2 = int(row['xmax']), int(row['ymax'])\n",
        "        conf, cls = row['confidence'], row['name']\n",
        "        label = f\"{cls} {conf:.2f}\"  # Format label with class and confidence\n",
        "\n",
        "        # Draw bounding box on the frame\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        # Add label above the bounding box\n",
        "        cv2.putText(frame, label, (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.5, (255, 0, 0), 2)\n",
        "\n",
        "    # Write the annotated frame to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release video resources after processing\n",
        "cap.release()  # Release input video\n",
        "out.release()  # Save the output video\n",
        "cv2.destroyAllWindows()  # Close OpenCV windows"
      ],
      "metadata": {
        "id": "rt20JYdncNl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 8-8 — Facial Landmark Extraction and Wireframe Rendering (Chapter Image Generation)\n",
        "\n",
        "This program demonstrates how AI systems detect and represent facial structure.  \n",
        "Using OpenCV’s 68-point landmark model, it identifies key reference points such as the eyes, nose, and mouth, then connects them into a simplified geometric mesh.  \n",
        "The resulting output shows two views: one with the wireframe overlaid on the original image, and another displaying the mesh alone as a transparent feature map ready for analysis or visualization."
      ],
      "metadata": {
        "id": "IvxgKzfpoZFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Remove conflicting packages you don't need for this listing\n",
        "!pip -q uninstall -y opencv-python opencv-python-headless opencv-contrib-python opencv-contrib-python-headless \\\n",
        "  albumentations albucore dopamine-rl thinc\n",
        "\n",
        "# 1) Install a stable set for the facemark demo\n",
        "!pip -q install \"numpy==1.26.4\" \"opencv-contrib-python==4.8.1.78\" \"matplotlib==3.8.4\"\n",
        "\n",
        "# 2) HARD restart the kernel so the new C-extensions load\n",
        "import os; os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "Flq_vfVwojJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request, os, tempfile\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration (adjust as needed)\n",
        "# -----------------------------\n",
        "USE_REMOTE   = True               # Set to False to use your own local image\n",
        "REMOTE_URL   = \"https://opensourceai-book.github.io/code/media/Superhero-FaceClone.png\"\n",
        "IMAGE_PATH   = \"face_input.png\"   # If using your own image, place it in the working dir with this name\n",
        "CANVAS_SIZE  = 500                # Output size for the aligned crop (square)\n",
        "MARGIN_FRAC  = 0.25               # How much margin around the detected face box\n",
        "\n",
        "# Wireframe style\n",
        "LINE_COLOR   = (0, 220, 90)       # Lines: green (BGR)\n",
        "LINE_THICK   = 2\n",
        "DOT_OUTLINE  = (30, 110, 255)     # Landmark outline: blue-ish (BGR)\n",
        "DOT_FILL     = (255, 255, 255)    # Landmark fill: white (BGR)\n",
        "DOT_R_OUT    = 6\n",
        "DOT_R_FILL   = 4\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load image (download or local)\n",
        "# -----------------------------\n",
        "if USE_REMOTE:\n",
        "    # Download to a generic name so readers can easily replace it later\n",
        "    urllib.request.urlretrieve(REMOTE_URL, IMAGE_PATH)\n",
        "\n",
        "img_bgr = cv2.imread(IMAGE_PATH)\n",
        "assert img_bgr is not None, f\"Could not load {IMAGE_PATH}. If using a local image, ensure it is named {IMAGE_PATH}.\"\n",
        "\n",
        "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Load 68-point LBF landmark model\n",
        "# -----------------------------\n",
        "lbf_url  = \"https://raw.githubusercontent.com/kurnianggoro/GSOC2017/master/data/lbfmodel.yaml\"\n",
        "lbf_path = os.path.join(tempfile.gettempdir(), \"lbfmodel.yaml\")\n",
        "if not os.path.exists(lbf_path):\n",
        "    urllib.request.urlretrieve(lbf_url, lbf_path)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Detect face, crop with margin, and resize to a canonical square\n",
        "# -----------------------------\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "faces = face_cascade.detectMultiScale(img_rgb, scaleFactor=1.1, minNeighbors=5, minSize=(80, 80))\n",
        "assert len(faces) > 0, \"No face found. Try a clearer, front-facing image.\"\n",
        "\n",
        "# Choose the largest detected face\n",
        "x, y, w, h = max(faces, key=lambda b: b[2] * b[3])\n",
        "\n",
        "H, W = img_rgb.shape[:2]\n",
        "m = int(MARGIN_FRAC * max(w, h))  # adjustable margin\n",
        "xh, yh = max(0, x - m), max(0, y - m)\n",
        "x2, y2 = min(W, x + w + m), min(H, y + h + m)\n",
        "\n",
        "crop_rgb = img_rgb[yh:y2, xh:x2]\n",
        "aligned_rgb = cv2.resize(crop_rgb, (CANVAS_SIZE, CANVAS_SIZE), interpolation=cv2.INTER_LINEAR)\n",
        "aligned_bgr = cv2.cvtColor(aligned_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Fit landmarks (68 points in iBUG layout)\n",
        "# -----------------------------\n",
        "facemark = cv2.face.createFacemarkLBF()\n",
        "facemark.loadModel(lbf_path)\n",
        "\n",
        "faces_aligned = face_cascade.detectMultiScale(aligned_rgb, 1.1, 5, minSize=(80, 80))\n",
        "assert len(faces_aligned) > 0, \"No face found in the aligned crop.\"\n",
        "\n",
        "ok, landmarks = facemark.fit(aligned_bgr, faces_aligned)\n",
        "assert ok and len(landmarks) > 0, \"Facemark fit failed.\"\n",
        "\n",
        "pts = landmarks[0][0].astype(np.int32)  # shape: (68, 2)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Build a light mesh (brows, eyes, nose, mouth) with minimal cross-links\n",
        "# -----------------------------\n",
        "def chain(seq, closed=False):\n",
        "    pairs = [(seq[i], seq[i + 1]) for i in range(len(seq) - 1)]\n",
        "    if closed:\n",
        "        pairs.append((seq[-1], seq[0]))\n",
        "    return pairs\n",
        "\n",
        "# 68-point iBUG indices\n",
        "browL = list(range(17, 22))\n",
        "browR = list(range(22, 27))\n",
        "eyeL  = list(range(36, 42))\n",
        "eyeR  = list(range(42, 48))\n",
        "noseB = list(range(27, 31))\n",
        "noseA = list(range(31, 36))\n",
        "lipO  = list(range(48, 60))  # outer lip\n",
        "lipI  = list(range(60, 68))  # inner lip\n",
        "\n",
        "# Light structure with a few cross-links. Tweak here for density.\n",
        "edges = []\n",
        "edges += chain(browL)\n",
        "edges += chain(browR)\n",
        "edges += chain(eyeL, closed=True)\n",
        "edges += chain(eyeR, closed=True)\n",
        "edges += chain(noseB)\n",
        "edges += chain(noseA)\n",
        "edges += chain(lipO, closed=True)\n",
        "edges += chain(lipI, closed=True)\n",
        "edges += [(21, 22), (39, 42), (27, 33), (33, 51), (33, 57)]  # minimal cross-links\n",
        "\n",
        "subset_idx = np.hstack([np.arange(17, 27), np.arange(27, 36),\n",
        "                        np.arange(36, 48), np.arange(48, 68)])\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Draw mesh on photo\n",
        "# -----------------------------\n",
        "on_photo = aligned_rgb.copy()\n",
        "for a, b in edges:\n",
        "    pa, pb = tuple(pts[a]), tuple(pts[b])\n",
        "    cv2.line(on_photo, pa, pb, LINE_COLOR, LINE_THICK, cv2.LINE_AA)\n",
        "\n",
        "for i in subset_idx:\n",
        "    px, py = pts[i]\n",
        "    cv2.circle(on_photo, (px, py), DOT_R_OUT, DOT_OUTLINE, 2, cv2.LINE_AA)\n",
        "    cv2.circle(on_photo, (px, py), DOT_R_FILL, DOT_FILL, -1, cv2.LINE_AA)\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Mesh-only (transparent PNG)\n",
        "# -----------------------------\n",
        "h, w = aligned_rgb.shape[:2]\n",
        "wire_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "\n",
        "for a, b in edges:\n",
        "    pa, pb = tuple(pts[a]), tuple(pts[b])\n",
        "    cv2.line(wire_rgb, pa, pb, LINE_COLOR, LINE_THICK, cv2.LINE_AA)\n",
        "\n",
        "for i in subset_idx:\n",
        "    px, py = pts[i]\n",
        "    cv2.circle(wire_rgb, (px, py), DOT_R_OUT, DOT_OUTLINE, 2, cv2.LINE_AA)\n",
        "    cv2.circle(wire_rgb, (px, py), DOT_R_FILL, DOT_FILL, -1, cv2.LINE_AA)\n",
        "\n",
        "alpha = cv2.cvtColor(wire_rgb, cv2.COLOR_RGB2GRAY)\n",
        "wire_rgba = np.zeros((h, w, 4), dtype=np.uint8)\n",
        "wire_rgba[..., :3] = wire_rgb\n",
        "wire_rgba[..., 3]  = (alpha > 0).astype(np.uint8) * 255  # stroke becomes alpha\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Display and save\n",
        "# -----------------------------\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax[0].imshow(on_photo)\n",
        "ax[0].set_title(\"Light mesh on photo\")\n",
        "ax[0].axis(\"off\")\n",
        "ax[1].imshow(wire_rgba)\n",
        "ax[1].set_title(\"Mesh-only (transparent)\")\n",
        "ax[1].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "cv2.imwrite(\"mesh_on_photo.png\", cv2.cvtColor(on_photo, cv2.COLOR_RGB2BGR))\n",
        "cv2.imwrite(\"mesh_only.png\", wire_rgba)\n",
        "print(\"Saved: mesh_on_photo.png, mesh_only.png\")"
      ],
      "metadata": {
        "id": "1HLterC3ohC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}