{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4daa6134",
      "metadata": {
        "id": "4daa6134"
      },
      "source": [
        "\n",
        "#### Climate Sentiment Classification with BERT\n",
        "\n",
        "In this notebook, we will go through the steps to train a sentiment analysis model using the BERT transformer model. We will:\n",
        "1. Load and prepare the dataset.\n",
        "2. Tokenize the text data.\n",
        "3. Split the data into training and evaluation sets.\n",
        "4. Train the model.\n",
        "5. Test the model on a set of predefined prompts.\n",
        "\n",
        "Let's begin!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install** Huggingface packages for both transformer models and datasets"
      ],
      "metadata": {
        "id": "ak-8lRi47BPY"
      },
      "id": "ak-8lRi47BPY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries for transformers and datasets from Hugging Face.\n",
        "# 'transformers' provides access to pre-trained models like BERT.\n",
        "# 'datasets' provides tools to easily load and process datasets.\n",
        "%pip -q install transformers datasets"
      ],
      "metadata": {
        "id": "rJSQI6TfAx55"
      },
      "id": "rJSQI6TfAx55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9c57e218",
      "metadata": {
        "id": "9c57e218"
      },
      "source": [
        "\n",
        "#### Step 1: Define Functions\n",
        "\n",
        "We will define the following functions to organize our code:\n",
        "*   `load_and_prepare_data`: Handles loading the dataset and getting it ready for tokenization.\n",
        "*   `tokenize_dataset`: Specifically takes care of converting our text data into a format that the BERT model can understand (tokenization).\n",
        "*   `select_subsets`: Helps us split the dataset into smaller portions for training and evaluation.\n",
        "*   `initialize_trainer`: Sets up the training environment, including the model, training parameters, and datasets.\n",
        "*   `test_model_performance`: Evaluates how well the model is doing on example sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjXVDl7k9nDx"
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset and prepare for training\n",
        "def load_and_prepare_data(file_path):\n",
        "    # Load the dataset from the specified file path.\n",
        "    # We specify 'csv' as the format and provide a dictionary mapping 'train' to the file path.\n",
        "    dataset = load_dataset('csv', data_files={'train': file_path})\n",
        "    # Tokenize the loaded dataset using the tokenize_dataset function.\n",
        "    # The map function applies the tokenize_dataset function to each example in the dataset.\n",
        "    # batched=True processes examples in batches, which is more efficient.\n",
        "    processed_dataset = dataset.map(tokenize_dataset, batched=True)\n",
        "    # Return the tokenized dataset.\n",
        "    return processed_dataset\n",
        "\n",
        "# Tokenize text data for BERT\n",
        "def tokenize_dataset(examples):\n",
        "    # Tokenize the 'text' field of the examples.\n",
        "    # padding=\"max_length\": Pads sequences to the maximum length specified by max_length.\n",
        "    # truncation=True: Truncates sequences that are longer than max_length.\n",
        "    # max_length=512: The maximum length of the tokenized sequences.\n",
        "    return bert_tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Select training and evaluation subsets\n",
        "def select_subsets(processed_dataset, subset_size_ratio=0.8):\n",
        "    # Determine the size of the subset to use, taking the minimum of 1000 and the total number of training examples.\n",
        "    subset_size = min(1200, len(processed_dataset['train']))\n",
        "    # Calculate the size of the training set based on the subset size and ratio.\n",
        "    train_size = int(subset_size * subset_size_ratio)\n",
        "\n",
        "    print(f\"Training set size: {train_size}\")\n",
        "    print(f\"Evaluation set size: {subset_size - train_size}\")\n",
        "\n",
        "    # Randomly shuffle the training dataset and select a subset for training.\n",
        "    # seed=42 ensures reproducibility of the shuffling.\n",
        "    train_subset = processed_dataset['train'].shuffle(seed=42).select(range(train_size))\n",
        "    # Randomly shuffle the training dataset and select a subset for evaluation (the remaining part after the training set).\n",
        "    eval_subset = processed_dataset['train'].shuffle(seed=42).select(range(train_size, subset_size))\n",
        "    # Return the small training and evaluation datasets.\n",
        "    return train_subset, eval_subset\n",
        "\n",
        "# Initialize and return the Trainer\n",
        "def initialize_trainer(train_dataset, eval_dataset):\n",
        "    # Define the training arguments.\n",
        "    training_args = TrainingArguments(\n",
        "        # Directory to save model checkpoints and outputs.\n",
        "        output_dir=\"/results\",\n",
        "        # The learning rate for the optimizer.\n",
        "        learning_rate=2e-5,\n",
        "        # Batch size for training on each device.\n",
        "        per_device_train_batch_size=8,\n",
        "        # Batch size for evaluation on each device.\n",
        "        per_device_eval_batch_size=8,\n",
        "        # Number of training epochs (set to 1 for demonstration, >1 for accurate results).\n",
        "        num_train_epochs=2,\n",
        "        # The weight decay to apply (L2 regularization).\n",
        "        weight_decay=0.01,\n",
        "        # Evaluate the model at the end of each epoch.\n",
        "        eval_strategy=\"epoch\",\n",
        "        # Disable logging to services like Weights & Biases.\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "    # Initialize the Trainer with the model, arguments, and datasets.\n",
        "    model_trainer = Trainer(\n",
        "        model=bert_model, # The BERT model to train.\n",
        "        args=training_args, # The training arguments.\n",
        "        train_dataset=train_dataset, # The training dataset.\n",
        "        eval_dataset=eval_dataset, # The evaluation dataset.\n",
        "    )\n",
        "    # Return the initialized trainer.\n",
        "    return model_trainer\n",
        "\n",
        "# Evaluate model performance on test prompts\n",
        "def test_model_performance(sentiment_pipeline, prompts):\n",
        "    # Define a mapping from the model's predicted labels to human-readable labels.\n",
        "    label_map = {'LABEL_0': 'Risk', 'LABEL_1': 'Neutral', 'LABEL_2': 'Opportunity'}\n",
        "    # Iterate through each prompt in the list.\n",
        "    for prompt in prompts:\n",
        "        # Get the sentiment prediction for the current prompt using the pipeline.\n",
        "        prediction = sentiment_pipeline(prompt)[0]\n",
        "        # Get the translated label from the label_map, defaulting to the original label if not found.\n",
        "        translated_label = label_map.get(prediction['label'], prediction['label'])\n",
        "        # Print the prompt, the predicted label, and the confidence score.\n",
        "        print(f\"Prompt: {prompt}\\nPrediction: Label: {translated_label}, Score: {prediction['score']}\\n\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "yjXVDl7k9nDx"
    },
    {
      "cell_type": "markdown",
      "id": "edd3da39",
      "metadata": {
        "id": "edd3da39"
      },
      "source": [
        "\n",
        "#### Step 2: Main Program\n",
        "\n",
        "Here, we initialize the tokenizer and model, load the dataset, and proceed with training the model. We will evaluate the model's performance before and after training on a set of predefined climate-related prompts.\n",
        "\n",
        "\n",
        "1.  **Initialize the Tokenizer and Model:** Load the pre-trained BERT tokenizer and the BERT model configured for our classification task.\n",
        "2.  **Load and Prepare Data:** Use our helper function to load the dataset and apply tokenization.\n",
        "3.  **Select Subsets:** Create smaller training and evaluation datasets from the loaded data.\n",
        "4.  **Initialize Trainer:** Set up the training process using the `Trainer` class and defined training arguments.\n",
        "5.  **Test Before Training:** Evaluate the model's performance on sample prompts *before* any training occurs to see the initial, untrained results.\n",
        "6.  **Train the Model:** Run the training process using the prepared data and trainer.\n",
        "7.  **Test After Training:** Evaluate the model's performance on the same sample prompts *after* training to observe the impact of the training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef78225b",
      "metadata": {
        "id": "ef78225b"
      },
      "outputs": [],
      "source": [
        "# Initialize the BERT tokenizer with the 'bert-base-uncased' model.\n",
        "# This tokenizer is used to preprocess text data for the BERT model.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Initialize the BERT model for sequence classification with 'bert-base-uncased'.\n",
        "# num_labels=3 specifies that the model should predict one of three classes (for sentiment).\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Check if the script is being run directly (not imported as a module).\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the path to the dataset file.\n",
        "    dataset_path = 'https://jerrycuomo.github.io/Think_Artificial_Intelligence/datasets/climatebert-climate-sentiment.csv'\n",
        "\n",
        "    # Load and prepare the dataset using the defined function.\n",
        "    tokenized_datasets = load_and_prepare_data(dataset_path)\n",
        "\n",
        "    # Select smaller training and evaluation subsets from the tokenized dataset.\n",
        "    small_train_dataset, small_eval_dataset = select_subsets(tokenized_datasets)\n",
        "\n",
        "    # Initialize the Trainer with the model and datasets.\n",
        "    trainer = initialize_trainer(small_train_dataset, small_eval_dataset)\n",
        "    print(\"Before Training:\")\n",
        "\n",
        "    # Create a sentiment analysis pipeline using the initialized model and tokenizer.\n",
        "    sentiment_pipeline_before = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    # Define a list of test prompts to evaluate the model.\n",
        "    test_prompts = [\n",
        "        \"The company has achieved a 20% reduction in water usage over the past year through improved conservation efforts.\",\n",
        "        \"Recent audits revealed non-compliance with environmental regulations in several of our manufacturing facilities.\",\n",
        "        \"Our new product line uses recycled materials, contributing to a circular economy and reducing waste.\",\n",
        "        \"Emissions have increased due to expanded operations.\",\n",
        "        \"The company is currently evaluating the environmental impact of its operations to better align with sustainability goals.\"\n",
        "    ]\n",
        "\n",
        "    # Test the model's performance on the test prompts before training.\n",
        "    test_model_performance(sentiment_pipeline_before, test_prompts)\n",
        "\n",
        "    # Train the model using the initialized trainer.\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"After Training:\")\n",
        "\n",
        "    # Create a new sentiment analysis pipeline with the trained model and tokenizer.\n",
        "    sentiment_pipeline_after = pipeline(\"sentiment-analysis\", model=trainer.model, tokenizer=tokenizer) # Use trainer.model here\n",
        "\n",
        "    # Test the model's performance on the test prompts after training.\n",
        "    test_model_performance(sentiment_pipeline_after, test_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c5933e",
      "metadata": {
        "id": "14c5933e"
      },
      "source": [
        "\n",
        "#### Conclusion\n",
        "\n",
        "In this notebook, we walked through the process of training a BERT model for sentiment analysis on climate-related data. We saw how to:\n",
        "1. Load and prepare the dataset.\n",
        "2. Tokenize the data for BERT.\n",
        "3. Train the model using the `Trainer` class.\n",
        "4. Evaluate the model's performance before and after training.\n",
        "\n",
        "Great work!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}