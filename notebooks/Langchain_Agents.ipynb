{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6g3ZvuKtXG0"
      },
      "source": [
        "\n",
        "### Installing Required Packages for Hugging Face and LangChain\n",
        "In this block, we install the necessary packages for using **Hugging Face Hub** and **LangChain**, which include community modules and tools for building language model applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqHl6daMtiFj"
      },
      "outputs": [],
      "source": [
        "# Install required packages for Hugging Face and LangChain usage\n",
        "\n",
        "print(\"Installing packages... this can take a minute or two.\")\n",
        "\n",
        "%pip install -q langchain langchain-community langchain-huggingface langchain-openai chromadb google-search-results\n",
        "\n",
        "print(\"All required packaged installed and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_AyNIIFt29R"
      },
      "source": [
        "### Setting Up Hugging Face Access Token\n",
        "We configure our environment with **access token** for Hugging Face, OpenAI and Google Search. This is necessary for programmatic access to models and datasets available on Hugging Face Hub, as well as access to OpenAI, and Google Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAMCtqThuAv6"
      },
      "outputs": [],
      "source": [
        "# Import os library for environment variable manipulation\n",
        "import os\n",
        "\n",
        "# Note: Ensure these are securely managed and not hard-coded in production.\n",
        "\n",
        "# Set your Hugging Face Hub Access token (replace 'your_token_here' with your actual token)\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"paste your huggingface access token here\"\n",
        "\n",
        "# Set OpenAI key for accessing GPT-3 and GPT-3 models (paid account)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"paste your openai access key here\"\n",
        "\n",
        "# SERPAPI API key for accessing Google Search API (Free account is limited to 100 API calls per month)\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"paste your google serpapi key here\"\n",
        "\n",
        "print(\"All Set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydWOhFwAvqgR"
      },
      "source": [
        "### Building a Simple Q&A Chatbot Using LangChain\n",
        "We will set up a basic **Q&A chatbot** using **LangChain** and a **small language model** from Hugging Face. This demonstrates chaining models and using templates.\n",
        "Exercises:\n",
        "- Experiment with different **small** models (uncomment LANGUAGE_MODEL to test alternatives).\n",
        "- Adjust the temperature setting (TEMP of 0.9 for the most varied responses, 0.1 for the least varied).\n",
        "- Try using different substitution variables (e.g., 'language':, set to \"Spanish\").\n",
        "- Now try **OpenAI's GPT** large language model (by uncomment corresponding line below)\n",
        "- Last, Alter Prompt to trigger rude response (e.g, ... you dummy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uppUVBb3wpdP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for building a LangChain chatbot\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# Initialize a small language model from Hugging Face\n",
        "#LANGUAGE_MODEL = \"meta-llama/Llama-2-7b\"\n",
        "#LANGUAGE_MODEL = \"google/flan-t5-small\"\n",
        "#LANGUAGE_MODEL = \"bigscience/bloom\" # Play with temperature to get different results\n",
        "#LANGUAGE_MODEL = \"tiiuae/falcon-7b-instruct\" # Works okay, best with temperature: 0.1-0.5\n",
        "LANGUAGE_MODEL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # Works okay with lower temperatures\n",
        "\n",
        "# Increase to cause vary reponses (.9 max, 0 min)\n",
        "TEMP = .2\n",
        "\n",
        "# Define LM to one fo the (smaller) Hugging Face models\n",
        "llm = HuggingFaceEndpoint( repo_id=LANGUAGE_MODEL, temperature=TEMP)\n",
        "\n",
        "# Uncomment to define OpenAI GPT model (A large language model)\n",
        "#llm = ChatOpenAI(temperature=.1)  # uncomment to use OpenAI GPT model (A large language model)\n",
        "\n",
        "# Define a prompt template for the chatbot\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please respond in {language} in 20 words or less. {validate}'),\n",
        "    ('human', '{input}')\n",
        "])\n",
        "\n",
        "# Define the chatbot chain where the prompt is sent to the language model\n",
        "chain = prompt | llm\n",
        "\n",
        "# Invoke the chatbot with a sample input\n",
        "response = chain.invoke({'input': 'Who is the tallest superhero?',\n",
        "                         'language': 'English',\n",
        "                         'validate': 'Keep it clean'})\n",
        "\n",
        "# Print the chatbot's response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2X1OiDRJ80T"
      },
      "source": [
        "### RAG-Based Document Summarization\n",
        "Demonstrates a **Retrieval-Augmented Generation** (RAG) process by splitting a document into chunks, embedding it into a searchable database, retrieving relevant information, and generating a summary using a language model.\n",
        "Exercises:\n",
        "- Change query_text, perhaps something to do with \"radon gas\".\n",
        "- Change Language to Spanish in prompt template\n",
        "- Update system prompt to include special formatting (E.g., HTML, JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFdA2MgTJ_jh"
      },
      "outputs": [],
      "source": [
        "# Import Embeddings model for RAG, and Chroma in memory vector database\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"langchain_text_splitters.base\").setLevel(logging.ERROR)\n",
        "\n",
        "# Load the document from a GITHUB, normalizing special characters\n",
        "DOC_URL = \"https://jerrycuomo.github.io/Think_Artificial_Intelligence/datasets/EPA-consumer-safety-safe-water.txt\"\n",
        "full_text = requests.get(DOC_URL).text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "# Chunk the document and tokenize\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300)\n",
        "texts = text_splitter.split_text(full_text)\n",
        "print(f\"Document has been split into {len(texts)} chunks\")\n",
        "\n",
        "# Initialize the embedding model and create a searchable database from the chunked texts\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_texts(texts, embeddings)\n",
        "\n",
        "# Retrieving the context from the DB using similarity search\n",
        "# query_text = \"Can Radon gas enter your home?\"\n",
        "query_text = \"What is considered safe drinking water?\"\n",
        "results = db.similarity_search(query_text, 1)\n",
        "\n",
        "# Configure the prompt template for concise summarization\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Please summarize in {language} in 25 words or less. {validate}\"),\n",
        "    (\"human\", \"{question} {input}\")\n",
        "])\n",
        "\n",
        "# Set up the LangChain LLM for processing the information retrieved, defining the sequence for action\n",
        "llm = ChatOpenAI(temperature=.2)\n",
        "chain = prompt | llm\n",
        "\n",
        "# Execute the chain on the first retrieved document, specifying the output language and summary style\n",
        "response = chain.invoke({\"question\": query_text,\n",
        "                         \"input\": results[0].page_content,\n",
        "                         \"language\": \"English\",\n",
        "                         \"validate\": \"Ensure clarity and accessibility\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpPKOXc8EBO7"
      },
      "source": [
        "### Langchain Agent (Skilled in Web Search and Math)\n",
        "This program initializes an Langchain-based agent equipped with search and math tools, allowing it to answer complex queries by retrieving information from the web and performing calculations dynamically.\n",
        "\n",
        "Exercise:\n",
        "- Try difference queries by uncommenting options below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI_tQT-UEGmC"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "\n",
        "# Initialize the OpenAI agent with a specific temperature setting\n",
        "llm = ChatOpenAI(temperature=.2)\n",
        "\n",
        "# Load necessary tools for the agent, including SERPAPI for searches and llm-math for mathematical queries\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Initialize the agent with the loaded tools, setting it to a zero-shot react description mode for dynamic response handling\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n",
        "\n",
        "# Define a query and invoke the agent to handle it, demonstrating the agent's capability to generate and evaluate responses\n",
        "#query = \"How much would it cost to fill a pool the size of an Olympic swimming pool using the average water price in Los Angeles?\"\n",
        "#query = \"Whatâ€™s the average monthly salary in Switzerland, and how long would it take a person earning that salary to save enough to buy a Tesla Model S, factoring in living costs of 70% of their income?\"\n",
        "#query = \"What's the current price of Tesla stock, and how much would 15 shares cost?\"\n",
        "\n",
        "query = \"What was the total score of the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "agent.invoke(query)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
