{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aaa9926"
      },
      "source": [
        "# Building Language Model Applications with LangChain and Hugging Face\n",
        "\n",
        "This notebook explores the use of **LangChain** and **Hugging Face** to build various language model applications. You will learn how to:\n",
        "\n",
        "- Set up your environment with necessary packages and API keys.\n",
        "- Build a simple Q&A chatbot using LangChain and a small language model from Hugging Face.\n",
        "- Implement a Retrieval-Augmented Generation (RAG) system for document summarization.\n",
        "- Utilize a LangChain agent equipped with tools for web search and calculations.\n",
        "- Introduce **CrewAI**, a framework for orchestrating collaborative autonomous agents.\n",
        "\n",
        "Each section includes code examples and exercises to help you experiment with different models, parameters, and techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6g3ZvuKtXG0"
      },
      "source": [
        "### Installing Required Packages for Hugging Face and LangChain\n",
        "In this block, we install the necessary packages for using **Hugging Face Hub**,  **LangChain**, and **more** which include community modules and tools for building language model applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqHl6daMtiFj"
      },
      "outputs": [],
      "source": [
        "# Install required packages for Hugging Face and LangChain usage\n",
        "\n",
        "%pip install -q \"langchain\" \"langchain-community\" \"langchain-huggingface\" \\\n",
        "                 \"langchain_openai\" \"huggingface_hub\" \"chromadb\" \"google-search-results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_AyNIIFt29R"
      },
      "source": [
        "### Setting Up Hugging Face Access Token\n",
        "We configure our environment with **access token** for Hugging Face, OpenAI and Google Search. This is necessary for programmatic access to models and datasets available on Hugging Face Hub, as well as access to OpenAI, and Google Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAMCtqThuAv6"
      },
      "outputs": [],
      "source": [
        "# Constants and API Key Configuration\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Load API keys securely from Google Colab Secrets ===\n",
        "def load_api_keys():\n",
        "    keys = {\n",
        "        \"HF_TOKEN\": userdata.get(\"HF_TOKEN\"),\n",
        "        \"OPENAI_API_KEY\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "        \"SERPAPI_API_KEY\": userdata.get(\"SERPAPI_API_KEY\"),\n",
        "        \"SERPER_API_KEY\": userdata.get(\"SERPER_API_KEY\")\n",
        "    }\n",
        "    for key, value in keys.items():\n",
        "        if not value:\n",
        "            raise ValueError(f\"❌ Missing {key}. Please set this API key in Colab secrets.\")\n",
        "        os.environ[key] = value\n",
        "    print(\"✅ All API keys loaded and configured successfully.\")\n",
        "\n",
        "# Execute API key loading upon running this cell\n",
        "load_api_keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydWOhFwAvqgR"
      },
      "source": [
        "### Building a Simple Q&A Chatbot Using LangChain\n",
        "We will set up a basic **Q&A chatbot** using **LangChain** and a **small language model** from Hugging Face. This demonstrates chaining models and using templates.\n",
        "Exercises:\n",
        "- Experiment with different **small** models (uncomment LANGUAGE_MODEL to test alternatives).\n",
        "- Adjust the temperature setting (TEMP of 0.9 for the most varied responses, 0.1 for the least varied).\n",
        "- Try using different substitution variables (e.g., 'language':, set to \"Spanish\").\n",
        "- Now try **OpenAI's GPT** large language model (by uncomment corresponding line below)\n",
        "- Last, Alter Prompt to trigger rude response (e.g, ... you dummy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidate Models\n",
        "\n",
        "#DEFAULT_MODEL = \"openai/gpt-oss-20b\"\n",
        "#DEFAULT_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "#DEFAULT_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "DEFAULT_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\""
      ],
      "metadata": {
        "id": "umbor4OKLFsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uppUVBb3wpdP"
      },
      "outputs": [],
      "source": [
        "# --- LangChain Chatbot ---\n",
        "\n",
        "# Import necessary libraries for the updated LangChain structure\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define the temperature\n",
        "TEMP = 0.5\n",
        "\n",
        "# --- Model and Chain Setup ---\n",
        "\n",
        "# 1. Define the base LLM (HuggingFaceEndpoint, equivalent to the original 'llm' instance)\n",
        "base_llm = HuggingFaceEndpoint(\n",
        "    repo_id=DEFAULT_MODEL,\n",
        "    temperature=TEMP,\n",
        "    # Setting max_new_tokens ~50-word limit\n",
        "    max_new_tokens=50,\n",
        "    # Optional: Set this to False if the model is chat-tuned; check model documentation.\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# 2A. Wrap the base LLM in ChatHuggingFace\n",
        "# This allows it to work seamlessly with ChatPromptTemplate messages\n",
        "chat_llm = ChatHuggingFace(llm=base_llm)\n",
        "\n",
        "# 2B. Wrap the base LLM in ChatOpenAI\n",
        "# chat_llm = ChatOpenAI(temperature=TEMP)\n",
        "\n",
        "# 3. Define the prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please respond in {language} in 25 words or less. {validate}'),\n",
        "    ('human',  'What is the capital of North Carolina?'), # Example for model to reference\n",
        "    ('ai', 'Raleigh is the capital of North Carolina.'),  # Corresponding answer from AI model\n",
        "    ('human', '{input}')\n",
        "])\n",
        "\n",
        "# 4. Define the chatbot chain\n",
        "# This uses the new structure: Prompt -> Chat LLM Wrapper -> Output Parser\n",
        "chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "# --- Invocation ---\n",
        "\n",
        "# Invoke the chatbot with the sample input\n",
        "response = chain.invoke({\n",
        "    'input': 'Who is the tallest superhero?',\n",
        "    'language': 'English',\n",
        "    'validate': 'Keep it clean'\n",
        "})\n",
        "\n",
        "# Print the chatbot's response (now guaranteed to be a clean string due to StrOutputParser)\n",
        "print(\"--- Response from Converted Chain ---\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2X1OiDRJ80T"
      },
      "source": [
        "### RAG-Based Document Summarization\n",
        "Demonstrates a **Retrieval-Augmented Generation** (RAG) process by splitting a document into chunks, embedding it into a searchable database, retrieving relevant information, and generating a summary using a language model.\n",
        "Exercises:\n",
        "- Change query_text, perhaps something to do with \"radon gas\".\n",
        "- Change Language to Spanish in prompt template\n",
        "- Update system prompt to include special formatting (E.g., HTML, JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFdA2MgTJ_jh"
      },
      "outputs": [],
      "source": [
        "# Import Embeddings model for RAG, and Chroma in memory vector database\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"langchain_text_splitters.base\").setLevel(logging.ERROR)\n",
        "\n",
        "# Load the document from a GITHUB, normalizing special characters\n",
        "DOC_URL = \"https://jerrycuomo.github.io/Think_Artificial_Intelligence/datasets/EPA-consumer-safety-safe-water.txt\"\n",
        "full_text = requests.get(DOC_URL).text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "# Chunk the document and tokenize\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300)\n",
        "texts = text_splitter.split_text(full_text)\n",
        "print(f\"Document has been split into {len(texts)} chunks\")\n",
        "\n",
        "# Initialize the embedding model and create a searchable database from the chunked texts\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_texts(texts, embeddings)\n",
        "\n",
        "# Question to ask the embeddig model\n",
        "# query_text = \"Can Radon gas enter your home?\"\n",
        "# query_text = \"What is considered safe drinking water?\"\n",
        "query_text = \"What is the issue with Arsenic?\"\n",
        "\n",
        "# Retrieving the context from the DB using similarity search\n",
        "results = db.similarity_search(query_text, 1)\n",
        "\n",
        "# Configure the prompt template for concise summarization\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Please summarize in {language} in 30 words or less. {validate}\"),\n",
        "    (\"human\", \"{question} {input}\")\n",
        "])\n",
        "\n",
        "# Set up the LangChain LLM for processing the information retrieved, defining the sequence for action\n",
        "llm = ChatOpenAI(temperature=.2)\n",
        "chain = prompt | llm\n",
        "\n",
        "# Execute the chain on the first retrieved document, specifying the output language and summary style\n",
        "response = chain.invoke({\"question\": query_text,\n",
        "                         \"input\": results[0].page_content,\n",
        "                         \"language\": \"English\",\n",
        "                         \"validate\": \"Say response in plain english.\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpPKOXc8EBO7"
      },
      "source": [
        "### Langchain Agent (Skilled in Web Search and Math)\n",
        "This program initializes an Langchain-based agent equipped with search and math tools, allowing it to answer complex queries by retrieving information from the web and performing calculations dynamically.\n",
        "\n",
        "Exercise:\n",
        "- Try difference queries by uncommenting options below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI_tQT-UEGmC"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "\n",
        "# Initialize the OpenAI agent with a specific temperature setting\n",
        "llm = ChatOpenAI(temperature=.2)\n",
        "\n",
        "# Load necessary tools for the agent, including SERPAPI for searches and llm-math for mathematical queries\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Initialize the agent with the loaded tools, setting it to a zero-shot react description mode for dynamic response handling\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n",
        "\n",
        "# Define a query and invoke the agent to handle it, demonstrating the agent's capability to generate and evaluate responses\n",
        "#query = \"How much would it cost to fill a pool the size of an Olympic swimming pool using the average water price in Los Angeles?\"\n",
        "#query = \"What’s the average monthly salary in Switzerland, and how long would it take a person earning that salary to save enough to buy a Tesla Model S, factoring in living costs of 70% of their income?\"\n",
        "#query = \"What's the current price of Tesla stock, and how much would 15 shares cost?\"\n",
        "#query = \"What should I wear today?\"\n",
        "\n",
        "query = \"What was the total score of the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "agent.invoke(query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d03e4f72"
      },
      "source": [
        "### CrewAI Agent: A Collaborative AI Framework\n",
        "This section introduces **CrewAI**, a framework for orchestrating autonomous agents that can work together to solve complex tasks. In this example, we set up a single agent equipped with a **web search tool** to answer user queries. The agent is designed with a specific **goal** and **backstory** to guide its decision-making process, particularly in determining when to use its tools. The code demonstrates how to:\n",
        "\n",
        "- Initialize a web search tool.\n",
        "- Define an agent with a role, goal, backstory, and assigned tools.\n",
        "- Create a task for the agent with a clear description and expected output.\n",
        "- Assemble a `Crew` with the defined agent and task.\n",
        "- Execute the crew with a user query and observe the agent's process and final answer.\n",
        "\n",
        "This example highlights how CrewAI allows for the creation of intelligent agents capable of deciding whether to use external tools based on the nature of the query and their defined objectives."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U --quiet 'crewai[tools]' aisuite"
      ],
      "metadata": {
        "id": "Fs47OQGNaBv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain_openai import ChatOpenAI\n",
        "from crewai_tools import WebsiteSearchTool\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 🔑 SETUP: Ensure your API Key is set as an environment variable (OPENAI_API_KEY)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Initialize the Search Tool\n",
        "# This is the agent's connection to real-time, external information.\n",
        "general_search_tool = WebsiteSearchTool()\n",
        "\n",
        "# Define the Language Model (LLM)\n",
        "# gpt-4o-mini is cost-effective and capable of complex reasoning.\n",
        "general_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 🧠 AGENT DEFINITION: The Intelligence Core\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "general_agent = Agent(\n",
        "    role=\"General Question Answerer\",\n",
        "    # The GOAL is the primary instruction for the agent's behavior.\n",
        "    goal=\"Answer any user query. Use the search tool only for questions requiring current or external information.\",\n",
        "    backstory=\"A simple assistant ready to answer questions.\",\n",
        "\n",
        "    tools=[general_search_tool],\n",
        "    llm=general_llm,\n",
        "    max_iter = 3, # Number of times to try to arrive at an answer\n",
        "    verbose=True, # Shows the agent's 'Thought' process (CoT reasoning)\n",
        "    allow_delegation=False\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 🎯 TASK DEFINITION: The CoT Instruction\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "answer_query_task = Task(\n",
        "    # The agent's goal and the LLM's natural reasoning will now guide the steps.\n",
        "    description=(\n",
        "        \"**Analyze the user query: '{query}' and provide the most complete and accurate answer possible.** \"\n",
        "        \"Use your tool to find any necessary current, real-time, or external data required to fulfill this request. \"\n",
        "        \"If the query requires context (like location or time) that is not provided, use your tools to deduce.\"\n",
        "    ),\n",
        "    expected_output=\"A single, concise, and accurate final answer to the user query.\",\n",
        "    agent=general_agent\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# ⚙️ CREW & EXECUTION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "research_crew = Crew(\n",
        "    agents=[general_agent],\n",
        "    tasks=[answer_query_task],\n",
        "    process=Process.sequential,\n",
        "    verbose=True # Shows the execution flow and final result\n",
        ")\n",
        "\n",
        "def run_lecture_demo(query: str):\n",
        "    \"\"\"Executes the CrewAI process for the given query.\"\"\"\n",
        "    if 'OPENAI_API_KEY' not in os.environ:\n",
        "        print(\"\\nERROR: Please set the OPENAI_API_KEY environment variable to run the demo.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n=====================================================\")\n",
        "    print(f\"🧠 RUNNING QUERY: **{query}**\")\n",
        "    print(\"=====================================================\")\n",
        "\n",
        "    user_inputs = {\"query\": query}\n",
        "\n",
        "    # Kickoff the crew execution\n",
        "    result = research_crew.kickoff(inputs=user_inputs)\n",
        "\n",
        "    print(\"\\n-----------------------------------------------------\")\n",
        "    print(f\"✅ FINAL ANSWER for '{query}':\\n\")\n",
        "    print(result)\n",
        "    print(\"-----------------------------------------------------\\n\")\n",
        "\n",
        "# --- DEMONSTRATION QUERIES ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 💡 Demo 1: Complex query that forces Deconstruction (CoT) and a Search.\n",
        "    # The agent must realize: 1) It needs current info (weather). 2) It needs context (location).\n",
        "    run_lecture_demo(\"What should I wear today?\")\n",
        "\n",
        "    # 💡 Demo 2 (Optional): Simple query that should use internal knowledge only.\n",
        "    # Uncomment to show the agent intelligently skipping the search tool.\n",
        "    # run_lecture_demo(\"Who wrote the novel 'Moby Dick'?\")"
      ],
      "metadata": {
        "id": "a2Xp1qniZ_bC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}