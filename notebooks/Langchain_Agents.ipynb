{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6g3ZvuKtXG0"
      },
      "source": [
        "### Installing Required Packages for Hugging Face and LangChain\n",
        "In this block, we install the necessary packages for using **Hugging Face Hub**,  **LangChain**, and **more** which include community modules and tools for building language model applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bqHl6daMtiFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ab9602-6fae-4139-abb3-ef20218a02ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for Hugging Face and LangChain usage\n",
        "\n",
        "%pip install -q \"langchain\" \"langchain-community\" \"langchain-huggingface\" \\\n",
        "                 \"langchain_openai\" \"huggingface_hub\" \"chromadb\" \"google-search-results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_AyNIIFt29R"
      },
      "source": [
        "### Setting Up Hugging Face Access Token\n",
        "We configure our environment with **access token** for Hugging Face, OpenAI and Google Search. This is necessary for programmatic access to models and datasets available on Hugging Face Hub, as well as access to OpenAI, and Google Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fAMCtqThuAv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e813a432-d5f3-4168-9942-f7dfd6341026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All API keys loaded and configured successfully.\n"
          ]
        }
      ],
      "source": [
        "# Constants and API Key Configuration\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# === Load API keys securely from Google Colab Secrets ===\n",
        "def load_api_keys():\n",
        "    keys = {\n",
        "        \"HF_TOKEN\": userdata.get(\"HF_TOKEN\"),\n",
        "        \"OPENAI_API_KEY\": userdata.get(\"OPENAI_API_KEY\"),\n",
        "        \"SERPAPI_API_KEY\": userdata.get(\"SERPAPI_API_KEY\")\n",
        "    }\n",
        "    for key, value in keys.items():\n",
        "        if not value:\n",
        "            raise ValueError(f\"❌ Missing {key}. Please set this API key in Colab secrets.\")\n",
        "        os.environ[key] = value\n",
        "    print(\"✅ All API keys loaded and configured successfully.\")\n",
        "\n",
        "# Execute API key loading upon running this cell\n",
        "load_api_keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydWOhFwAvqgR"
      },
      "source": [
        "### Building a Simple Q&A Chatbot Using LangChain\n",
        "We will set up a basic **Q&A chatbot** using **LangChain** and a **small language model** from Hugging Face. This demonstrates chaining models and using templates.\n",
        "Exercises:\n",
        "- Experiment with different **small** models (uncomment LANGUAGE_MODEL to test alternatives).\n",
        "- Adjust the temperature setting (TEMP of 0.9 for the most varied responses, 0.1 for the least varied).\n",
        "- Try using different substitution variables (e.g., 'language':, set to \"Spanish\").\n",
        "- Now try **OpenAI's GPT** large language model (by uncomment corresponding line below)\n",
        "- Last, Alter Prompt to trigger rude response (e.g, ... you dummy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidate Models\n",
        "\n",
        "#DEFAULT_MODEL = \"openai/gpt-oss-20b\"\n",
        "#DEFAULT_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "#DEFAULT_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "DEFAULT_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\""
      ],
      "metadata": {
        "id": "umbor4OKLFsy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uppUVBb3wpdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1e38eb-d4bb-4cb3-ea7d-592d2c34422b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Response from Converted Chain ---\n",
            "The tallest superhero is Giant-Man, also known as Ant-Man or Goliath, who can grow up to 100 feet tall.\n"
          ]
        }
      ],
      "source": [
        "# --- LangChain Chatbot ---\n",
        "\n",
        "# Import necessary libraries for the updated LangChain structure\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define the temperature\n",
        "TEMP = 0.5\n",
        "\n",
        "# --- Model and Chain Setup (The 'New Way') ---\n",
        "\n",
        "# 1. Define the base LLM (HuggingFaceEndpoint, equivalent to the original 'llm' instance)\n",
        "base_llm = HuggingFaceEndpoint(\n",
        "    repo_id=DEFAULT_MODEL,\n",
        "    temperature=TEMP,\n",
        "    # Setting max_new_tokens ~50-word limit\n",
        "    max_new_tokens=50,\n",
        "    # Optional: Set this to False if the model is chat-tuned; check model documentation.\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# 2A. Wrap the base LLM in ChatHuggingFace\n",
        "# This allows it to work seamlessly with ChatPromptTemplate messages\n",
        "chat_llm = ChatHuggingFace(llm=base_llm)\n",
        "\n",
        "# 2B. Wrap the base LLM in ChatOpenAI\n",
        "# chat_llm = ChatOpenAI(temperature=TEMP)\n",
        "\n",
        "# 3. Define the prompt template (Exactly the same as the 'Old Way')\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'Please respond in {language} in 25 words or less. {validate}'),\n",
        "    ('human', '{input}')\n",
        "])\n",
        "\n",
        "# 4. Define the chatbot chain\n",
        "# This uses the new structure: Prompt -> Chat LLM Wrapper -> Output Parser\n",
        "chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "# --- Invocation ---\n",
        "\n",
        "# Invoke the chatbot with the sample input\n",
        "response = chain.invoke({\n",
        "    'input': 'Who is the tallest superhero?',\n",
        "    'language': 'English',\n",
        "    'validate': 'Keep it clean'\n",
        "})\n",
        "\n",
        "# Print the chatbot's response (now guaranteed to be a clean string due to StrOutputParser)\n",
        "print(\"--- Response from Converted Chain ---\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2X1OiDRJ80T"
      },
      "source": [
        "### RAG-Based Document Summarization\n",
        "Demonstrates a **Retrieval-Augmented Generation** (RAG) process by splitting a document into chunks, embedding it into a searchable database, retrieving relevant information, and generating a summary using a language model.\n",
        "Exercises:\n",
        "- Change query_text, perhaps something to do with \"radon gas\".\n",
        "- Change Language to Spanish in prompt template\n",
        "- Update system prompt to include special formatting (E.g., HTML, JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bFdA2MgTJ_jh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48330361-eb25-4a6a-88ba-2f44815023d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document has been split into 113 chunks\n",
            "Drinking water may contain contaminants like microbes, chemicals, and metals. Regulations by EPA and FDA ensure safe tap and bottled water by limiting certain contaminants. Contact EPA for more information.\n"
          ]
        }
      ],
      "source": [
        "# Import Embeddings model for RAG, and Chroma in memory vector database\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"langchain_text_splitters.base\").setLevel(logging.ERROR)\n",
        "\n",
        "# Load the document from a GITHUB, normalizing special characters\n",
        "DOC_URL = \"https://jerrycuomo.github.io/Think_Artificial_Intelligence/datasets/EPA-consumer-safety-safe-water.txt\"\n",
        "full_text = requests.get(DOC_URL).text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "# Chunk the document and tokenize\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300)\n",
        "texts = text_splitter.split_text(full_text)\n",
        "print(f\"Document has been split into {len(texts)} chunks\")\n",
        "\n",
        "# Initialize the embedding model and create a searchable database from the chunked texts\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_texts(texts, embeddings)\n",
        "\n",
        "# Retrieving the context from the DB using similarity search\n",
        "# query_text = \"Can Radon gas enter your home?\"\n",
        "query_text = \"What is considered safe drinking water?\"\n",
        "results = db.similarity_search(query_text, 1)\n",
        "\n",
        "# Configure the prompt template for concise summarization\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Please summarize in {language} in 30 words or less. {validate}\"),\n",
        "    (\"human\", \"{question} {input}\")\n",
        "])\n",
        "\n",
        "# Set up the LangChain LLM for processing the information retrieved, defining the sequence for action\n",
        "llm = ChatOpenAI(temperature=.2)\n",
        "chain = prompt | llm\n",
        "\n",
        "# Execute the chain on the first retrieved document, specifying the output language and summary style\n",
        "response = chain.invoke({\"question\": query_text,\n",
        "                         \"input\": results[0].page_content,\n",
        "                         \"language\": \"English\",\n",
        "                         \"validate\": \"Say response in plain english.\"})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpPKOXc8EBO7"
      },
      "source": [
        "### Langchain Agent (Skilled in Web Search and Math)\n",
        "This program initializes an Langchain-based agent equipped with search and math tools, allowing it to answer complex queries by retrieving information from the web and performing calculations dynamically.\n",
        "\n",
        "Exercise:\n",
        "- Try difference queries by uncommenting options below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rI_tQT-UEGmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d5e399-c93c-484e-a3ee-7db18613f783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3214460978.py:11: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find out the total score of the Super Bowl in the year Justin Bieber was born.\n",
            "Action: Search\n",
            "Action Input: Super Bowl total score in [Justin Bieber birth year]\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['Justin Bieber ; Born. Justin Drew Bieber. (1994-03-01) March 1, 1994 (age 31). London, Ontario, Canada ; Occupations. Singer; songwriter ; Years active, 2007– ...', \"Justin Bieber ranks No. 8 on Billboard's Super Bowl Halftime Show Top Picks: “Following the surprise drop of his R&B-leaning Swag album in ...\", 'The singer made eight points, four assists and two rebounds for the West team, which lost the game 54-49.', 'The Super Bowl was a star-studded event. From Jay-Z to Justin Bieber, see all the celebrities who attended. The Chiefs and 49ers brought out ...', 'There is no gig in music like the Super Bowl halftime show. You have 15 minutes to justify your legend. You have 150 million people watching ...', \"It's worth remembering, after all, that Bieber was just 15 when he scored his first Top 10 record. Now an industry veteran at the age of 22, Bieber is a top ...\", \"Super Bowl Synch Roundup: Lady Gaga & Justin Bieber Lead a Down Year — Although Execs 'Are Closing Deals Until the Last Minute'. Examining the ...\", 'Justin Bieber was born on March 1, 1994, in London, Ontario, Canada. He was discovered on YouTube at age 14 by Scooter Braun, who flew him to ...', 'Justin Randall Timberlake (born January 31, 1981) is an American singer, songwriter, actor, record producer, and dancer. Dubbed the \"Prince of Pop\", ...', 'The results speak for themselves: 23 GRAMMY nominations with two wins, eight No.1 albums, eight No. 1 Billboard Hot 100 hits, and 89 million monthly Spotify ...']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe search results did not provide the specific total score of the Super Bowl in Justin Bieber's birth year.\n",
            "Action: Search\n",
            "Action Input: Super Bowl total score in 1994\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m['The Cowboys defeated the Bills, for the second straight year, by a score of 30–13, winning their fourth Super Bowl in team history.', 'Game summary of the Dallas Cowboys vs. Buffalo Bills NFL game, final score 30-13, from January 30, 1994 on ESPN.', 'The Cowboys defeated the Bills by the score of 52–17, winning their third Super Bowl in team history, and their first one in 15 years.', 'View a comprehensive list of every single NFL Super Bowl champion from 1967 to present on ESPN. Includes the finals opponent, site of the game and the final', 'The Dallas Cowboys were down 13-6 at halftime of Super Bowl XXVIII. But they came back strong, scoring 24 unanswered points to win 30-13 and capture their ...', 'The Dallas Cowboys easily beat the Buffalo Bills, 30 to 13, in Super Bowl XXVIII on January 30, 1994. Interpreted as: cowboys vs last bills super bowl score.', 'Dallas Cowboys 30 vs. Buffalo Bills 13 on January 30th, 1994 - Full team and player stats and box score.', 'Super Bowl 1994 was held in Atlanta at the Georgia Dome ... Final Score: Dallas Cowboys (NFC) – 30 Vs. Buffalo Bills (AFC) – 13. Date ...', 'The Cowboys defeated the Bills by the score of 30–13, winning their fourth Super Bowl in team history, tying the Pittsburgh Steelers and the San Francisco 49ers ...', '8, 26. San Francisco, 14, 14, 14, 7, 49. Super Bowl XXIX Box Score. Super Bowl XXVIII. January 30, 1994 ... FINAL. Kansas City, 10. Green Bay, 35. Super Bowl I ...']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe total score of the Super Bowl in 1994 was 30-13.\n",
            "Final Answer: 43\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What was the total score of the Super Bowl in the year Justin Bieber was born?',\n",
              " 'output': '43'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import load_tools, initialize_agent, AgentType\n",
        "\n",
        "# Initialize the OpenAI agent with a specific temperature setting\n",
        "llm = ChatOpenAI(temperature=.2)\n",
        "\n",
        "# Load necessary tools for the agent, including SERPAPI for searches and llm-math for mathematical queries\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Initialize the agent with the loaded tools, setting it to a zero-shot react description mode for dynamic response handling\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n",
        "\n",
        "# Define a query and invoke the agent to handle it, demonstrating the agent's capability to generate and evaluate responses\n",
        "#query = \"How much would it cost to fill a pool the size of an Olympic swimming pool using the average water price in Los Angeles?\"\n",
        "#query = \"What’s the average monthly salary in Switzerland, and how long would it take a person earning that salary to save enough to buy a Tesla Model S, factoring in living costs of 70% of their income?\"\n",
        "#query = \"What's the current price of Tesla stock, and how much would 15 shares cost?\"\n",
        "\n",
        "query = \"What was the total score of the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "agent.invoke(query)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}